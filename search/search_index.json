{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"pdldb","text":"<p>A high-performance analytical data store combining Polars' processing speed with Delta Lake's ACID transactions. This lightweight wrapper provides a database-like experience for local data processing.</p>"},{"location":"#overview","title":"Overview","text":"<p>pdldb creates a columnar data store that offers:</p> <ul> <li>The speed and efficiency of Polars for data operations</li> <li>The reliability and versioning of Delta Lake for data integrity</li> <li>Simple database-like operations with table management</li> <li>Flexible storage options with both local and cloud-based implementations</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install pdldb\n</code></pre>"},{"location":"#example-table-life-cycle","title":"Example Table Life Cycle","text":"<pre><code>import polars as pl\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom pdldb import LocalLakeManager\n\n\n# 1. Generate sample data\nn_rows = int(10000)\nnp.random.seed(7)\nstart_date = datetime(2025, 3, 23)\n\ndata = {\n    \"sequence\": np.arange(1, n_rows + 1, dtype=np.int32),\n    \"id\": np.random.permutation(n_rows) + 1,\n    \"value_1\": np.random.normal(100, 15, n_rows).astype(np.float32),\n    \"value_2\": np.random.uniform(0, 1000, n_rows).astype(np.float32),\n    \"value_3\": np.random.choice([\"A\", \"B\", \"C\", \"D\"], n_rows),\n    \"value_4\": np.random.exponential(50, n_rows).astype(np.float32),\n    \"value_5\": [(start_date + timedelta(seconds=i)) for i in range(n_rows)],\n}\n\ndf = pl.DataFrame(data)\n\n# Split data for demonstration purposes (overlapping rows)\ndf_part_one = df.slice(0, 6000)\ndf_part_two = df.slice(5000, 5000)\n\n# 2. Initialize Delta Lake\nlake = LocalLakeManager(\"pdldb_demo\")\n\n# 3. Define schema\nschema = {\n    \"sequence\": pl.Int32,\n    \"id\": pl.Int64,\n    \"value_1\": pl.Float32,\n    \"value_2\": pl.Float32,\n    \"value_3\": pl.Utf8,\n    \"value_4\": pl.Float32,\n    \"value_5\": pl.Datetime(\"ns\"),\n}\n\n# 4. Create table and load initial data\nlake.create_table(\"my_table\", schema, primary_keys=[\"sequence\", \"value_5\"])\nlake.append_table(\"my_table\", df_part_one)\n\n# 5. Read data from table\ninitial_data = lake.get_data_frame(\"my_table\")\n\n# 6. Add new data (ignoring duplicate records)\nlake.merge_table(\"my_table\", df_part_two, merge_condition=\"insert\")\nmerged_data = lake.get_data_frame(\"my_table\")\n\n# 7. Query full table with SQL and lazy evaluation\nldf = lake.get_lazy_frame(\"my_table\")\nresult = ldf.sql(\"SELECT sequence, id FROM self WHERE sequence &gt; 100\").collect()\n\n# 8. Get table metadata\ntables = lake.list_tables()\ntable_info = lake.get_table_info(\"my_table\")\nschema = lake.get_table_schema(\"my_table\")\n\n# 9. Maintenance operations\nlake.optimize_table(\"my_table\")\nlake.vacuum_table(\"my_table\")\n\n# 10. Overwrite table\nsmall_df = df.slice(0, 50)\nlake.overwrite_table(\"my_table\", small_df)\nafter_overwrite = lake.get_data_frame(\"my_table\")\n\n# 11. Delete table\nlake.delete_table(\"my_table\")\n</code></pre> <p>More examples can be found in the example folder</p>"},{"location":"api/api-reference/","title":"API Reference","text":"<p>This page provides auto-generated API documentation from docstrings.</p>"},{"location":"api/api-reference/#lake-managers","title":"Lake Managers","text":"<p>Core components for managing Delta Lake tables.</p>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager","title":"<code>pdldb.lake_manager.LakeManager</code>","text":"<p>Base class for managing a data lake with tables stored in Delta format.</p> <p>This class provides the foundation for creating, reading, updating, and managing Delta tables in a data lake. It's designed to be extended by specific implementations like LocalLakeManager.</p> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>class LakeManager:\n    \"\"\"\n    Base class for managing a data lake with tables stored in Delta format.\n\n    This class provides the foundation for creating, reading, updating, and managing\n    Delta tables in a data lake. It's designed to be extended by specific implementations\n    like LocalLakeManager.\n    \"\"\"\n\n    def __init__(\n        self, base_path: str, storage_options: Optional[Dict[str, Any]] = None\n    ):\n        \"\"\"\n        Initialize a new LakeManager.\n\n        Args:\n            base_path: The base path where the data lake will be stored\n            storage_options: Optional cloud storage-specific parameters\n        \"\"\"\n        params = LakeManagerInitModel(\n            base_path=base_path, storage_options=storage_options\n        )\n        if params.base_path.startswith(\"s3://\"):\n            self.base_path = params.base_path\n        else:\n            self.base_path = Path(params.base_path)\n\n        if isinstance(self.base_path, str):\n            if not self.base_path.endswith(\"/\"):\n                self.base_path += \"/\"\n        else:\n            path_str = str(self.base_path)\n            if not path_str.endswith(os.path.sep):\n                self.base_path = Path(f\"{path_str}{os.path.sep}\")\n\n        self.storage_options = params.storage_options\n        self.table_manager = None\n\n    def _check_table_exists(self, table_name: str) -&gt; None:\n        if table_name not in self.table_manager.tables:\n            raise ValueError(f\"Table {table_name} does not exist\")\n\n    def _check_table_not_exists(self, table_name: str) -&gt; None:\n        if table_name in self.table_manager.tables:\n            raise ValueError(f\"Table {table_name} already exists\")\n\n    def create_table(\n        self,\n        table_name: str,\n        table_schema: Dict[str, Any],\n        primary_keys: Union[str, List[str]],\n    ) -&gt; None:\n        \"\"\"\n        Create a new table in the data lake.\n\n        Args:\n            table_name: Name of the table to create\n            table_schema: Schema definition for the new table\n            primary_keys: Primary key column(s) for the table\n\n        Notes:\n            - The schema is enforced for write operations.\n            - The primary keys are used to identify unique records for merge operations.\n            - The primary key can be a single column or a composite key (multiple columns).\n            - Primary keys can be specified as a string or a list of strings.\n\n        Example: Single primary key\n            ```python\n            from pdldb import LocalLakeManager\n            import polars as pl\n\n            lake_manager = LocalLakeManager(\"data\")\n            schema = {\n                \"sequence\": pl.Int32,\n                \"value_1\": pl.Float64,\n                \"value_2\": pl.Utf8,\n                \"value_3\": pl.Float64,\n                \"value_4\": pl.Float64,\n                \"value_5\": pl.Datetime(\"ns\"),\n            }\n            primary_keys = \"sequence\"\n            lake_manager.create_table(\"my_table\", schema, primary_keys)\n            ```\n\n        Example: Composite primary key\n            ```python\n            primary_keys = [\"sequence\", \"value_1\"]\n            lake_manager.create_table(\"my_table\", schema, primary_keys)\n            ```\n        \"\"\"\n        params = TableCreateModel(\n            table_name=table_name, table_schema=table_schema, primary_keys=primary_keys\n        )\n\n        self._check_table_not_exists(table_name=params.table_name)\n        self.table_manager.create_table(\n            table_name=params.table_name,\n            table_schema=params.table_schema,\n            primary_keys=params.primary_keys,\n        )\n\n    def append_table(\n        self,\n        table_name: str,\n        df: pl.DataFrame,\n        delta_write_options: Optional[Dict[str, Any]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Append data to an existing table.\n\n        Args:\n            table_name: Name of the table to append to\n            df: DataFrame containing the data to append\n            delta_write_options: Optional configuration for the delta write operation\n\n        Notes:\n            - The schema of the DataFrame must match the schema of the table\n            - Appending data to a table has been intialized but contains no data will create the table on your storage backend.\n\n        Example:\n            ```python\n            lake_manager.append_table(\"my_table\", newdata)\n            ```\n        \"\"\"\n        params = TableOperationModel(\n            table_name=table_name, df=df, delta_write_options=delta_write_options\n        )\n\n        self._check_table_exists(table_name=params.table_name)\n        self.table_manager.append(\n            table_name=params.table_name,\n            df=params.df,\n            delta_write_options=params.delta_write_options,\n        )\n\n    def merge_table(\n        self,\n        table_name: str,\n        df: pl.DataFrame,\n        merge_condition: str = \"insert\",\n        delta_write_options: Optional[Dict[str, Any]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Merge data into an existing table based on the specified merge condition.\n\n        Args:\n            table_name: Name of the table to merge data into\n            df: DataFrame containing the data to merge\n            merge_condition: Type of merge operation to perform (update, insert, delete, upsert, upsert_delete)\n            delta_write_options: Optional configuration for the delta write operation\n\n        merge_condition:\n            - update: Update existing rows only from the new data\n            - insert: Insert new rows only from the new data\n            - delete: Delete existing rows that exist in the new data\n            - upsert: Update existing rows and insert new rows from the new data\n            - upsert_delete: Update existing rows, insert new rows, and delete rows that don't exist in the new data\n\n        Notes:\n            - If the table has been intialized but contains no data, merge operations requiring existing data ('update', 'delete', 'upsert_delete') will fail with an error message.\n            - The 'insert' and upsert' operations will create the table on your storage backend if the table has been intialized but contains no data.\n            - Primary keys defined for the table are used to determine matching records.\n\n        Example:\n            ```python\n            lake_manager.merge_table(\"my_table\", new_data, merge_condition=\"upsert\")\n            ```\n        \"\"\"\n        params = MergeOperationModel(\n            table_name=table_name,\n            df=df,\n            merge_condition=merge_condition,\n            delta_write_options=delta_write_options,\n        )\n\n        self._check_table_exists(table_name=params.table_name)\n\n        self.table_manager.merge(\n            table_name=params.table_name,\n            df=params.df,\n            delta_write_options=params.delta_write_options,\n            merge_condition=params.merge_condition,\n        )\n\n    def overwrite_table(\n        self,\n        table_name: str,\n        df: pl.DataFrame,\n        delta_write_options: Optional[Dict[str, Any]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Overwrite an existing table with new data.\n\n        Args:\n            table_name: Name of the table to overwrite\n            df: DataFrame containing the new data\n            delta_write_options: Optional configuration for the delta write operation\n\n        Notes:\n            - The schema of the DataFrame must match the schema of the table\n            - Overwriting a table that has been intialized but contains no data will create the table on your storage backend.\n            - Overwriting a table with existing data will replace the entire table.\n\n        Example:\n            ```python\n            lake_manager.overwrite_table(\"my_table\", new_data)\n            ```\n        \"\"\"\n        params = TableOperationModel(\n            table_name=table_name, df=df, delta_write_options=delta_write_options\n        )\n\n        self._check_table_exists(table_name=params.table_name)\n        self.table_manager.overwrite(\n            table_name=params.table_name,\n            df=params.df,\n            delta_write_options=params.delta_write_options,\n        )\n\n    def get_data_frame(self, table_name: str) -&gt; pl.DataFrame:\n        \"\"\"\n        Get an eager DataFrame from a table.\n\n        Args:\n            table_name: Name of the table to read\n\n        Returns:\n            A Polars DataFrame containing the table data\n\n        Notes:\n            - All table data is loaded into a Polars DataFrame in memory.\n            - This is suitable for small to medium-sized tables.\n\n        Example:\n            ```python\n            df = lake_manager.get_data_frame(\"my_table\")\n            ```\n        \"\"\"\n        params = TableNameModel(table_name=table_name)\n        self._check_table_exists(table_name=params.table_name)\n        return self.table_manager.get_data_frame(table_name=params.table_name)\n\n    def get_lazy_frame(self, table_name: str) -&gt; pl.LazyFrame:\n        \"\"\"\n        Get a lazy DataFrame from a table for deferred execution.\n\n        Args:\n            table_name: Name of the table to read\n\n        Returns:\n            A Polars LazyFrame referencing the table data\n\n        Notes:\n            - LazyFrames allow for deferred execution and optimization of query plans.\n            - Table data is not loaded into memory until an action (like collect) is called.\n            - This is suitable for large tables or complex queries.\n\n        Example:\n            ```python\n            lazy_frame = lake_manager.get_lazy_frame(\"my_table\")\n            result = lazy_frame.filter(col(\"column\") &gt; 10).select([\"column\"])\n            result.collect()\n            ```\n        \"\"\"\n        params = TableNameModel(table_name=table_name)\n        self._check_table_exists(table_name=params.table_name)\n        return self.table_manager.get_lazy_frame(table_name=params.table_name)\n\n    def optimize_table(\n        self,\n        table_name: str,\n        target_size: int = 512 * 1024 * 1024,\n        max_concurrent_tasks: Optional[int] = None,\n        writer_properties: Optional[Dict[str, Any]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Optimize a table by compacting small files in to files of the target size.\n        Optimizing a table can improve query performance and cloud costs.\n\n        Args:\n            table_name: Name of the table to optimize\n            target_size: Target file size in bytes for optimization\n            max_concurrent_tasks: Maximum number of concurrent tasks for optimization\n            writer_properties: Optional writer properties for optimization\n\n        Notes:\n            - The target size is the desired size of the output files after optimization.\n            - The default target size is 512 MB (512 * 1024 * 1024 bytes).\n            - The optimization process may take some time depending on the size of the table and the number of files.\n\n        Example:\n            ```python\n            lake_manager.optimize_table(\"my_table\", target_size=512*1024*1024)\n            ```\n        \"\"\"\n        params = OptimizeTableModel(\n            table_name=table_name,\n            target_size=target_size,\n            max_concurrent_tasks=max_concurrent_tasks,\n            writer_properties=writer_properties,\n        )\n\n        self._check_table_exists(table_name=params.table_name)\n        self.table_manager.optimize_table(\n            table_name=params.table_name,\n            target_size=params.target_size,\n            max_concurrent_tasks=params.max_concurrent_tasks,\n            writer_properties=params.writer_properties,\n        )\n\n    def vacuum_table(\n        self,\n        table_name: str,\n        retention_hours: Optional[int] = 168,\n        enforce_retention_duration: Optional[bool] = False,\n    ) -&gt; None:\n        \"\"\"\n        Clean up old data files from a table based on the retention period.\n        Old data files are those that are no longer referenced by the table.\n\n        Args:\n            table_name: Name of the table to vacuum\n            retention_hours: Retention period in hours (0 means delete all unreferenced files)\n            enforce_retention_duration: Whether to enforce the retention period\n\n        Notes:\n            - The retention period is the time duration for which files are retained.\n            - Files older than the retention period will be deleted.\n            - Setting retention_hours to 0 will delete all unreferenced files, regardless of age.\n            - The enforce_retention_duration flag ensures that the retention period is strictly enforced.\n            - Use caution when setting retention_hours to 0, as this will delete all unreferenced files.\n            - This operation is irreversible, deleted files cannot be recovered.\n            - The vacuum operation may take some time depending on the size of the table and the number of files.\n\n        Example:\n            ```python\n            lake_manager.vacuum_table(\"my_table\", retention_hours=24)\n            ```\n        \"\"\"\n        params = VacuumTableModel(\n            table_name=table_name,\n            retention_hours=retention_hours,\n            enforce_retention_duration=enforce_retention_duration,\n        )\n\n        self._check_table_exists(table_name=params.table_name)\n        self.table_manager.vacuum_table(\n            table_name=params.table_name,\n            retention_hours=params.retention_hours,\n            enforce_retention_duration=params.enforce_retention_duration,\n        )\n\n    def list_tables(self) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"\n        List all tables in the data lake.\n\n        Returns:\n            A dictionary mapping table names to their metadata\n\n        Example:\n            ```python\n            lake_manager.list_tables()\n            ```\n        \"\"\"\n        return self.table_manager.list_tables()\n\n    def get_table_info(self, table_name: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get detailed information about a specific table.\n\n        Args:\n            table_name: Name of the table to get information for\n\n        Returns:\n            A dictionary containing detailed table information\n\n        Example:\n            ```python\n            lake_manager.get_table_info(\"my_table\")\n            ```\n        \"\"\"\n        params = TableNameModel(table_name=table_name)\n        self._check_table_exists(table_name=params.table_name)\n        return self.table_manager.get_table_info(table_name=params.table_name)\n\n    def get_table_schema(self, table_name: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get the schema definition for a specific table.\n\n        Args:\n            table_name: Name of the table to get the schema for\n\n        Returns:\n            A dictionary representing the table schema\n\n        Example:\n            ```python\n            lake_manager.get_table_schema(\"my_table\")\n            ```\n        \"\"\"\n        params = TableNameModel(table_name=table_name)\n        self._check_table_exists(table_name=params.table_name)\n        return self.table_manager.get_table_schema(table_name=params.table_name)\n\n    def delete_table(self, table_name: str) -&gt; bool:\n        \"\"\"\n        Delete a table from the data lake.\n        Deleted data files are not recoverable, so use with caution.\n\n        Args:\n            table_name: Name of the table to delete\n\n        Returns:\n            True if the table was successfully deleted\n\n        Notes:\n            - This operation is irreversible, and deleted tables cannot be recovered.\n            - Use caution when deleting tables, especially in production environments.\n            - Ensure that you have backups or copies of important data before deletion.\n            - Deleting a table will remove all associated data files and metadata.\n\n        Example:\n            ```python\n            lake_manager.delete_table(\"my_table\")\n            ```\n        \"\"\"\n        params = TableNameModel(table_name=table_name)\n        self._check_table_exists(table_name=params.table_name)\n        return self.table_manager.delete_table(table_name=params.table_name)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager-functions","title":"Functions","text":""},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.__init__","title":"<code>__init__(base_path, storage_options=None)</code>","text":"<p>Initialize a new LakeManager.</p> PARAMETER DESCRIPTION <code>base_path</code> <p>The base path where the data lake will be stored</p> <p> TYPE: <code>str</code> </p> <code>storage_options</code> <p>Optional cloud storage-specific parameters</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def __init__(\n    self, base_path: str, storage_options: Optional[Dict[str, Any]] = None\n):\n    \"\"\"\n    Initialize a new LakeManager.\n\n    Args:\n        base_path: The base path where the data lake will be stored\n        storage_options: Optional cloud storage-specific parameters\n    \"\"\"\n    params = LakeManagerInitModel(\n        base_path=base_path, storage_options=storage_options\n    )\n    if params.base_path.startswith(\"s3://\"):\n        self.base_path = params.base_path\n    else:\n        self.base_path = Path(params.base_path)\n\n    if isinstance(self.base_path, str):\n        if not self.base_path.endswith(\"/\"):\n            self.base_path += \"/\"\n    else:\n        path_str = str(self.base_path)\n        if not path_str.endswith(os.path.sep):\n            self.base_path = Path(f\"{path_str}{os.path.sep}\")\n\n    self.storage_options = params.storage_options\n    self.table_manager = None\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.append_table","title":"<code>append_table(table_name, df, delta_write_options=None)</code>","text":"<p>Append data to an existing table.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to append to</p> <p> TYPE: <code>str</code> </p> <code>df</code> <p>DataFrame containing the data to append</p> <p> TYPE: <code>DataFrame</code> </p> <code>delta_write_options</code> <p>Optional configuration for the delta write operation</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> Notes <ul> <li>The schema of the DataFrame must match the schema of the table</li> <li>Appending data to a table has been intialized but contains no data will create the table on your storage backend.</li> </ul> Example <pre><code>lake_manager.append_table(\"my_table\", newdata)\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def append_table(\n    self,\n    table_name: str,\n    df: pl.DataFrame,\n    delta_write_options: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"\n    Append data to an existing table.\n\n    Args:\n        table_name: Name of the table to append to\n        df: DataFrame containing the data to append\n        delta_write_options: Optional configuration for the delta write operation\n\n    Notes:\n        - The schema of the DataFrame must match the schema of the table\n        - Appending data to a table has been intialized but contains no data will create the table on your storage backend.\n\n    Example:\n        ```python\n        lake_manager.append_table(\"my_table\", newdata)\n        ```\n    \"\"\"\n    params = TableOperationModel(\n        table_name=table_name, df=df, delta_write_options=delta_write_options\n    )\n\n    self._check_table_exists(table_name=params.table_name)\n    self.table_manager.append(\n        table_name=params.table_name,\n        df=params.df,\n        delta_write_options=params.delta_write_options,\n    )\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.create_table","title":"<code>create_table(table_name, table_schema, primary_keys)</code>","text":"<p>Create a new table in the data lake.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to create</p> <p> TYPE: <code>str</code> </p> <code>table_schema</code> <p>Schema definition for the new table</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>primary_keys</code> <p>Primary key column(s) for the table</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> Notes <ul> <li>The schema is enforced for write operations.</li> <li>The primary keys are used to identify unique records for merge operations.</li> <li>The primary key can be a single column or a composite key (multiple columns).</li> <li>Primary keys can be specified as a string or a list of strings.</li> </ul> Single primary key <pre><code>from pdldb import LocalLakeManager\nimport polars as pl\n\nlake_manager = LocalLakeManager(\"data\")\nschema = {\n    \"sequence\": pl.Int32,\n    \"value_1\": pl.Float64,\n    \"value_2\": pl.Utf8,\n    \"value_3\": pl.Float64,\n    \"value_4\": pl.Float64,\n    \"value_5\": pl.Datetime(\"ns\"),\n}\nprimary_keys = \"sequence\"\nlake_manager.create_table(\"my_table\", schema, primary_keys)\n</code></pre> Composite primary key <pre><code>primary_keys = [\"sequence\", \"value_1\"]\nlake_manager.create_table(\"my_table\", schema, primary_keys)\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def create_table(\n    self,\n    table_name: str,\n    table_schema: Dict[str, Any],\n    primary_keys: Union[str, List[str]],\n) -&gt; None:\n    \"\"\"\n    Create a new table in the data lake.\n\n    Args:\n        table_name: Name of the table to create\n        table_schema: Schema definition for the new table\n        primary_keys: Primary key column(s) for the table\n\n    Notes:\n        - The schema is enforced for write operations.\n        - The primary keys are used to identify unique records for merge operations.\n        - The primary key can be a single column or a composite key (multiple columns).\n        - Primary keys can be specified as a string or a list of strings.\n\n    Example: Single primary key\n        ```python\n        from pdldb import LocalLakeManager\n        import polars as pl\n\n        lake_manager = LocalLakeManager(\"data\")\n        schema = {\n            \"sequence\": pl.Int32,\n            \"value_1\": pl.Float64,\n            \"value_2\": pl.Utf8,\n            \"value_3\": pl.Float64,\n            \"value_4\": pl.Float64,\n            \"value_5\": pl.Datetime(\"ns\"),\n        }\n        primary_keys = \"sequence\"\n        lake_manager.create_table(\"my_table\", schema, primary_keys)\n        ```\n\n    Example: Composite primary key\n        ```python\n        primary_keys = [\"sequence\", \"value_1\"]\n        lake_manager.create_table(\"my_table\", schema, primary_keys)\n        ```\n    \"\"\"\n    params = TableCreateModel(\n        table_name=table_name, table_schema=table_schema, primary_keys=primary_keys\n    )\n\n    self._check_table_not_exists(table_name=params.table_name)\n    self.table_manager.create_table(\n        table_name=params.table_name,\n        table_schema=params.table_schema,\n        primary_keys=params.primary_keys,\n    )\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.delete_table","title":"<code>delete_table(table_name)</code>","text":"<p>Delete a table from the data lake. Deleted data files are not recoverable, so use with caution.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to delete</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if the table was successfully deleted</p> Notes <ul> <li>This operation is irreversible, and deleted tables cannot be recovered.</li> <li>Use caution when deleting tables, especially in production environments.</li> <li>Ensure that you have backups or copies of important data before deletion.</li> <li>Deleting a table will remove all associated data files and metadata.</li> </ul> Example <pre><code>lake_manager.delete_table(\"my_table\")\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def delete_table(self, table_name: str) -&gt; bool:\n    \"\"\"\n    Delete a table from the data lake.\n    Deleted data files are not recoverable, so use with caution.\n\n    Args:\n        table_name: Name of the table to delete\n\n    Returns:\n        True if the table was successfully deleted\n\n    Notes:\n        - This operation is irreversible, and deleted tables cannot be recovered.\n        - Use caution when deleting tables, especially in production environments.\n        - Ensure that you have backups or copies of important data before deletion.\n        - Deleting a table will remove all associated data files and metadata.\n\n    Example:\n        ```python\n        lake_manager.delete_table(\"my_table\")\n        ```\n    \"\"\"\n    params = TableNameModel(table_name=table_name)\n    self._check_table_exists(table_name=params.table_name)\n    return self.table_manager.delete_table(table_name=params.table_name)\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.get_data_frame","title":"<code>get_data_frame(table_name)</code>","text":"<p>Get an eager DataFrame from a table.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to read</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>A Polars DataFrame containing the table data</p> Notes <ul> <li>All table data is loaded into a Polars DataFrame in memory.</li> <li>This is suitable for small to medium-sized tables.</li> </ul> Example <pre><code>df = lake_manager.get_data_frame(\"my_table\")\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def get_data_frame(self, table_name: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Get an eager DataFrame from a table.\n\n    Args:\n        table_name: Name of the table to read\n\n    Returns:\n        A Polars DataFrame containing the table data\n\n    Notes:\n        - All table data is loaded into a Polars DataFrame in memory.\n        - This is suitable for small to medium-sized tables.\n\n    Example:\n        ```python\n        df = lake_manager.get_data_frame(\"my_table\")\n        ```\n    \"\"\"\n    params = TableNameModel(table_name=table_name)\n    self._check_table_exists(table_name=params.table_name)\n    return self.table_manager.get_data_frame(table_name=params.table_name)\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.get_lazy_frame","title":"<code>get_lazy_frame(table_name)</code>","text":"<p>Get a lazy DataFrame from a table for deferred execution.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to read</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>LazyFrame</code> <p>A Polars LazyFrame referencing the table data</p> Notes <ul> <li>LazyFrames allow for deferred execution and optimization of query plans.</li> <li>Table data is not loaded into memory until an action (like collect) is called.</li> <li>This is suitable for large tables or complex queries.</li> </ul> Example <pre><code>lazy_frame = lake_manager.get_lazy_frame(\"my_table\")\nresult = lazy_frame.filter(col(\"column\") &gt; 10).select([\"column\"])\nresult.collect()\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def get_lazy_frame(self, table_name: str) -&gt; pl.LazyFrame:\n    \"\"\"\n    Get a lazy DataFrame from a table for deferred execution.\n\n    Args:\n        table_name: Name of the table to read\n\n    Returns:\n        A Polars LazyFrame referencing the table data\n\n    Notes:\n        - LazyFrames allow for deferred execution and optimization of query plans.\n        - Table data is not loaded into memory until an action (like collect) is called.\n        - This is suitable for large tables or complex queries.\n\n    Example:\n        ```python\n        lazy_frame = lake_manager.get_lazy_frame(\"my_table\")\n        result = lazy_frame.filter(col(\"column\") &gt; 10).select([\"column\"])\n        result.collect()\n        ```\n    \"\"\"\n    params = TableNameModel(table_name=table_name)\n    self._check_table_exists(table_name=params.table_name)\n    return self.table_manager.get_lazy_frame(table_name=params.table_name)\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.get_table_info","title":"<code>get_table_info(table_name)</code>","text":"<p>Get detailed information about a specific table.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to get information for</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code> <p>A dictionary containing detailed table information</p> Example <pre><code>lake_manager.get_table_info(\"my_table\")\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def get_table_info(self, table_name: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get detailed information about a specific table.\n\n    Args:\n        table_name: Name of the table to get information for\n\n    Returns:\n        A dictionary containing detailed table information\n\n    Example:\n        ```python\n        lake_manager.get_table_info(\"my_table\")\n        ```\n    \"\"\"\n    params = TableNameModel(table_name=table_name)\n    self._check_table_exists(table_name=params.table_name)\n    return self.table_manager.get_table_info(table_name=params.table_name)\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.get_table_schema","title":"<code>get_table_schema(table_name)</code>","text":"<p>Get the schema definition for a specific table.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to get the schema for</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code> <p>A dictionary representing the table schema</p> Example <pre><code>lake_manager.get_table_schema(\"my_table\")\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def get_table_schema(self, table_name: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get the schema definition for a specific table.\n\n    Args:\n        table_name: Name of the table to get the schema for\n\n    Returns:\n        A dictionary representing the table schema\n\n    Example:\n        ```python\n        lake_manager.get_table_schema(\"my_table\")\n        ```\n    \"\"\"\n    params = TableNameModel(table_name=table_name)\n    self._check_table_exists(table_name=params.table_name)\n    return self.table_manager.get_table_schema(table_name=params.table_name)\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.list_tables","title":"<code>list_tables()</code>","text":"<p>List all tables in the data lake.</p> RETURNS DESCRIPTION <code>Dict[str, Dict[str, Any]]</code> <p>A dictionary mapping table names to their metadata</p> Example <pre><code>lake_manager.list_tables()\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def list_tables(self) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"\n    List all tables in the data lake.\n\n    Returns:\n        A dictionary mapping table names to their metadata\n\n    Example:\n        ```python\n        lake_manager.list_tables()\n        ```\n    \"\"\"\n    return self.table_manager.list_tables()\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.merge_table","title":"<code>merge_table(table_name, df, merge_condition='insert', delta_write_options=None)</code>","text":"<p>Merge data into an existing table based on the specified merge condition.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to merge data into</p> <p> TYPE: <code>str</code> </p> <code>df</code> <p>DataFrame containing the data to merge</p> <p> TYPE: <code>DataFrame</code> </p> <code>merge_condition</code> <p>Type of merge operation to perform (update, insert, delete, upsert, upsert_delete)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'insert'</code> </p> <code>delta_write_options</code> <p>Optional configuration for the delta write operation</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> merge_condition <ul> <li>update: Update existing rows only from the new data</li> <li>insert: Insert new rows only from the new data</li> <li>delete: Delete existing rows that exist in the new data</li> <li>upsert: Update existing rows and insert new rows from the new data</li> <li>upsert_delete: Update existing rows, insert new rows, and delete rows that don't exist in the new data</li> </ul> Notes <ul> <li>If the table has been intialized but contains no data, merge operations requiring existing data ('update', 'delete', 'upsert_delete') will fail with an error message.</li> <li>The 'insert' and upsert' operations will create the table on your storage backend if the table has been intialized but contains no data.</li> <li>Primary keys defined for the table are used to determine matching records.</li> </ul> Example <pre><code>lake_manager.merge_table(\"my_table\", new_data, merge_condition=\"upsert\")\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def merge_table(\n    self,\n    table_name: str,\n    df: pl.DataFrame,\n    merge_condition: str = \"insert\",\n    delta_write_options: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"\n    Merge data into an existing table based on the specified merge condition.\n\n    Args:\n        table_name: Name of the table to merge data into\n        df: DataFrame containing the data to merge\n        merge_condition: Type of merge operation to perform (update, insert, delete, upsert, upsert_delete)\n        delta_write_options: Optional configuration for the delta write operation\n\n    merge_condition:\n        - update: Update existing rows only from the new data\n        - insert: Insert new rows only from the new data\n        - delete: Delete existing rows that exist in the new data\n        - upsert: Update existing rows and insert new rows from the new data\n        - upsert_delete: Update existing rows, insert new rows, and delete rows that don't exist in the new data\n\n    Notes:\n        - If the table has been intialized but contains no data, merge operations requiring existing data ('update', 'delete', 'upsert_delete') will fail with an error message.\n        - The 'insert' and upsert' operations will create the table on your storage backend if the table has been intialized but contains no data.\n        - Primary keys defined for the table are used to determine matching records.\n\n    Example:\n        ```python\n        lake_manager.merge_table(\"my_table\", new_data, merge_condition=\"upsert\")\n        ```\n    \"\"\"\n    params = MergeOperationModel(\n        table_name=table_name,\n        df=df,\n        merge_condition=merge_condition,\n        delta_write_options=delta_write_options,\n    )\n\n    self._check_table_exists(table_name=params.table_name)\n\n    self.table_manager.merge(\n        table_name=params.table_name,\n        df=params.df,\n        delta_write_options=params.delta_write_options,\n        merge_condition=params.merge_condition,\n    )\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.optimize_table","title":"<code>optimize_table(table_name, target_size=512 * 1024 * 1024, max_concurrent_tasks=None, writer_properties=None)</code>","text":"<p>Optimize a table by compacting small files in to files of the target size. Optimizing a table can improve query performance and cloud costs.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to optimize</p> <p> TYPE: <code>str</code> </p> <code>target_size</code> <p>Target file size in bytes for optimization</p> <p> TYPE: <code>int</code> DEFAULT: <code>512 * 1024 * 1024</code> </p> <code>max_concurrent_tasks</code> <p>Maximum number of concurrent tasks for optimization</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>writer_properties</code> <p>Optional writer properties for optimization</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> Notes <ul> <li>The target size is the desired size of the output files after optimization.</li> <li>The default target size is 512 MB (512 * 1024 * 1024 bytes).</li> <li>The optimization process may take some time depending on the size of the table and the number of files.</li> </ul> Example <pre><code>lake_manager.optimize_table(\"my_table\", target_size=512*1024*1024)\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def optimize_table(\n    self,\n    table_name: str,\n    target_size: int = 512 * 1024 * 1024,\n    max_concurrent_tasks: Optional[int] = None,\n    writer_properties: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"\n    Optimize a table by compacting small files in to files of the target size.\n    Optimizing a table can improve query performance and cloud costs.\n\n    Args:\n        table_name: Name of the table to optimize\n        target_size: Target file size in bytes for optimization\n        max_concurrent_tasks: Maximum number of concurrent tasks for optimization\n        writer_properties: Optional writer properties for optimization\n\n    Notes:\n        - The target size is the desired size of the output files after optimization.\n        - The default target size is 512 MB (512 * 1024 * 1024 bytes).\n        - The optimization process may take some time depending on the size of the table and the number of files.\n\n    Example:\n        ```python\n        lake_manager.optimize_table(\"my_table\", target_size=512*1024*1024)\n        ```\n    \"\"\"\n    params = OptimizeTableModel(\n        table_name=table_name,\n        target_size=target_size,\n        max_concurrent_tasks=max_concurrent_tasks,\n        writer_properties=writer_properties,\n    )\n\n    self._check_table_exists(table_name=params.table_name)\n    self.table_manager.optimize_table(\n        table_name=params.table_name,\n        target_size=params.target_size,\n        max_concurrent_tasks=params.max_concurrent_tasks,\n        writer_properties=params.writer_properties,\n    )\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.overwrite_table","title":"<code>overwrite_table(table_name, df, delta_write_options=None)</code>","text":"<p>Overwrite an existing table with new data.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to overwrite</p> <p> TYPE: <code>str</code> </p> <code>df</code> <p>DataFrame containing the new data</p> <p> TYPE: <code>DataFrame</code> </p> <code>delta_write_options</code> <p>Optional configuration for the delta write operation</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> Notes <ul> <li>The schema of the DataFrame must match the schema of the table</li> <li>Overwriting a table that has been intialized but contains no data will create the table on your storage backend.</li> <li>Overwriting a table with existing data will replace the entire table.</li> </ul> Example <pre><code>lake_manager.overwrite_table(\"my_table\", new_data)\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def overwrite_table(\n    self,\n    table_name: str,\n    df: pl.DataFrame,\n    delta_write_options: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"\n    Overwrite an existing table with new data.\n\n    Args:\n        table_name: Name of the table to overwrite\n        df: DataFrame containing the new data\n        delta_write_options: Optional configuration for the delta write operation\n\n    Notes:\n        - The schema of the DataFrame must match the schema of the table\n        - Overwriting a table that has been intialized but contains no data will create the table on your storage backend.\n        - Overwriting a table with existing data will replace the entire table.\n\n    Example:\n        ```python\n        lake_manager.overwrite_table(\"my_table\", new_data)\n        ```\n    \"\"\"\n    params = TableOperationModel(\n        table_name=table_name, df=df, delta_write_options=delta_write_options\n    )\n\n    self._check_table_exists(table_name=params.table_name)\n    self.table_manager.overwrite(\n        table_name=params.table_name,\n        df=params.df,\n        delta_write_options=params.delta_write_options,\n    )\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.vacuum_table","title":"<code>vacuum_table(table_name, retention_hours=168, enforce_retention_duration=False)</code>","text":"<p>Clean up old data files from a table based on the retention period. Old data files are those that are no longer referenced by the table.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to vacuum</p> <p> TYPE: <code>str</code> </p> <code>retention_hours</code> <p>Retention period in hours (0 means delete all unreferenced files)</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>168</code> </p> <code>enforce_retention_duration</code> <p>Whether to enforce the retention period</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>False</code> </p> Notes <ul> <li>The retention period is the time duration for which files are retained.</li> <li>Files older than the retention period will be deleted.</li> <li>Setting retention_hours to 0 will delete all unreferenced files, regardless of age.</li> <li>The enforce_retention_duration flag ensures that the retention period is strictly enforced.</li> <li>Use caution when setting retention_hours to 0, as this will delete all unreferenced files.</li> <li>This operation is irreversible, deleted files cannot be recovered.</li> <li>The vacuum operation may take some time depending on the size of the table and the number of files.</li> </ul> Example <pre><code>lake_manager.vacuum_table(\"my_table\", retention_hours=24)\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def vacuum_table(\n    self,\n    table_name: str,\n    retention_hours: Optional[int] = 168,\n    enforce_retention_duration: Optional[bool] = False,\n) -&gt; None:\n    \"\"\"\n    Clean up old data files from a table based on the retention period.\n    Old data files are those that are no longer referenced by the table.\n\n    Args:\n        table_name: Name of the table to vacuum\n        retention_hours: Retention period in hours (0 means delete all unreferenced files)\n        enforce_retention_duration: Whether to enforce the retention period\n\n    Notes:\n        - The retention period is the time duration for which files are retained.\n        - Files older than the retention period will be deleted.\n        - Setting retention_hours to 0 will delete all unreferenced files, regardless of age.\n        - The enforce_retention_duration flag ensures that the retention period is strictly enforced.\n        - Use caution when setting retention_hours to 0, as this will delete all unreferenced files.\n        - This operation is irreversible, deleted files cannot be recovered.\n        - The vacuum operation may take some time depending on the size of the table and the number of files.\n\n    Example:\n        ```python\n        lake_manager.vacuum_table(\"my_table\", retention_hours=24)\n        ```\n    \"\"\"\n    params = VacuumTableModel(\n        table_name=table_name,\n        retention_hours=retention_hours,\n        enforce_retention_duration=enforce_retention_duration,\n    )\n\n    self._check_table_exists(table_name=params.table_name)\n    self.table_manager.vacuum_table(\n        table_name=params.table_name,\n        retention_hours=params.retention_hours,\n        enforce_retention_duration=params.enforce_retention_duration,\n    )\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LocalLakeManager","title":"<code>pdldb.lake_manager.LocalLakeManager</code>","text":"<p>               Bases: <code>LakeManager</code></p> <p>Implementation of LakeManager for local filesystem storage.</p> <p>This class extends the base LakeManager to provide specific functionality for managing Delta tables in a local filesystem.</p> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>class LocalLakeManager(LakeManager):\n    \"\"\"\n    Implementation of LakeManager for local filesystem storage.\n\n    This class extends the base LakeManager to provide specific functionality\n    for managing Delta tables in a local filesystem.\n    \"\"\"\n\n    def __init__(self, base_path: str):\n        \"\"\"\n        Initialize a new LocalLakeManager.\n\n        Args:\n            base_path: The local filesystem path where the data lake will be stored\n\n        Example:\n            ```python\n            from pdldb import LocalLakeManager\n            lake_manager = LocalLakeManager(\"data\")\n            ```\n        \"\"\"\n        params = LakeManagerInitModel(base_path=base_path, storage_options=None)\n        super().__init__(params.base_path, params.storage_options)\n        self.base_path.mkdir(parents=True, exist_ok=True)\n        self.table_manager = LocalTableManager(self.base_path, self.storage_options)\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LocalLakeManager-functions","title":"Functions","text":""},{"location":"api/api-reference/#pdldb.lake_manager.LocalLakeManager.__init__","title":"<code>__init__(base_path)</code>","text":"<p>Initialize a new LocalLakeManager.</p> PARAMETER DESCRIPTION <code>base_path</code> <p>The local filesystem path where the data lake will be stored</p> <p> TYPE: <code>str</code> </p> Example <pre><code>from pdldb import LocalLakeManager\nlake_manager = LocalLakeManager(\"data\")\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def __init__(self, base_path: str):\n    \"\"\"\n    Initialize a new LocalLakeManager.\n\n    Args:\n        base_path: The local filesystem path where the data lake will be stored\n\n    Example:\n        ```python\n        from pdldb import LocalLakeManager\n        lake_manager = LocalLakeManager(\"data\")\n        ```\n    \"\"\"\n    params = LakeManagerInitModel(base_path=base_path, storage_options=None)\n    super().__init__(params.base_path, params.storage_options)\n    self.base_path.mkdir(parents=True, exist_ok=True)\n    self.table_manager = LocalTableManager(self.base_path, self.storage_options)\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.S3LakeManager","title":"<code>pdldb.lake_manager.S3LakeManager</code>","text":"<p>               Bases: <code>LakeManager</code></p> <p>Implementation of LakeManager for Amazon S3 storage.</p> <p>This class extends the base LakeManager to provide specific functionality for managing Delta tables in Amazon S3.</p> Notes <p>Delta Lake normally guarantees ACID transactions when writing data; this is done by default when writing to all supported object stores except AWS S3.</p> <p>When writing to S3, there are two approaches:</p> <ol> <li> <p>Using a DynamoDB locking provider (recommended for production):    This ensures safe concurrent writes (ACID) by using a DynamoDB table to manage locks.</p> </li> <li> <p>Allowing unsafe renames:    This approach doesn't guarantee data consistency with concurrent writes.</p> </li> </ol> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>class S3LakeManager(LakeManager):\n    \"\"\"\n    Implementation of LakeManager for Amazon S3 storage.\n\n    This class extends the base LakeManager to provide specific functionality\n    for managing Delta tables in Amazon S3.\n\n    Notes:\n        Delta Lake normally guarantees ACID transactions when writing data; this is done\n        by default when writing to all supported object stores except AWS S3.\n\n        When writing to S3, there are two approaches:\n\n        1. Using a DynamoDB locking provider (recommended for production):\n           This ensures safe concurrent writes (ACID) by using a DynamoDB table to manage locks.\n\n        2. Allowing unsafe renames:\n           This approach doesn't guarantee data consistency with concurrent writes.\n    \"\"\"\n\n    def __init__(\n        self,\n        base_path: str,\n        aws_region: str,\n        aws_access_key: str,\n        aws_secret_key: str,\n        dynamodb_locking_table: Optional[str] = None,\n    ):\n        \"\"\"\n        Initialize a new S3LakeManager.\n\n        Args:\n            base_path: The S3 bucket path where the data lake will be stored (e.g., \"s3://bucket/prefix/\")\n            aws_region: AWS region name (e.g., \"us-east-1\")\n            aws_access_key: AWS access key ID\n            aws_secret_key: AWS secret access key\n            dynamodb_locking_table: Optional name of DynamoDB table to use as locking provider.\n                                    If not provided, unsafe renames will be enabled.\n\n        Notes:\n            - For production use with concurrent writes, it's strongly recommended to provide\n              a DynamoDB table for locking to ensure ACID guarantees.\n            - The DynamoDB table must have the following schema:\n              - Partition key: 'tablePath' (String)\n              - Sort key: 'fileName' (String)\n            - If no dynamodb_locking_table is provided, the S3LakeManager will use unsafe\n              renames which doesn't guarantee data consistency with concurrent writes.\n            - You can create the required DynamoDB table with:\n              ```console\n              aws dynamodb create-table\n                  --table-name delta_log\n                  --attribute-definitions AttributeName=tablePath,AttributeType=S AttributeName=fileName,AttributeType=S\n                  --key-schema AttributeName=tablePath,KeyType=HASH AttributeName=fileName,KeyType=RANGE\n                  --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5\n              ```\n            - Consider setting a TTL on the DynamoDB table to avoid it growing indefinitely.\n\n        Example:\n            ```python\n            from pdldb import S3LakeManager\n\n            # Using unsafe renames (not safe for concurrent writes)\n            lake_manager = S3LakeManager(\n                \"s3://mybucket/mydatalake/\",\n                aws_region=\"us-east-1\",\n                aws_access_key=\"YOUR_ACCESS_KEY\",\n                aws_secret_key=\"YOUR_SECRET_KEY\"\n            )\n\n            # Using DynamoDB locking provider (safe for concurrent writes)\n            lake_manager = S3LakeManager(\n                \"s3://mybucket/mydatalake/\",\n                aws_region=\"us-east-1\",\n                aws_access_key=\"YOUR_ACCESS_KEY\",\n                aws_secret_key=\"YOUR_SECRET_KEY\",\n                dynamodb_locking_table=\"delta_log\"\n            )\n            ```\n        \"\"\"\n        storage_options = {\n            \"AWS_REGION\": aws_region,\n            \"AWS_ACCESS_KEY_ID\": aws_access_key,\n            \"AWS_SECRET_ACCESS_KEY\": aws_secret_key,\n        }\n\n        if dynamodb_locking_table:\n            storage_options[\"AWS_S3_LOCKING_PROVIDER\"] = \"dynamodb\"\n            storage_options[\"DELTA_DYNAMO_TABLE_NAME\"] = dynamodb_locking_table\n        else:\n            storage_options[\"AWS_S3_ALLOW_UNSAFE_RENAME\"] = \"true\"\n\n        params = LakeManagerInitModel(\n            base_path=base_path, storage_options=storage_options\n        )\n        super().__init__(params.base_path, params.storage_options)\n        self.table_manager = S3TableManager(str(self.base_path), self.storage_options)\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.S3LakeManager-functions","title":"Functions","text":""},{"location":"api/api-reference/#pdldb.lake_manager.S3LakeManager.__init__","title":"<code>__init__(base_path, aws_region, aws_access_key, aws_secret_key, dynamodb_locking_table=None)</code>","text":"<p>Initialize a new S3LakeManager.</p> PARAMETER DESCRIPTION <code>base_path</code> <p>The S3 bucket path where the data lake will be stored (e.g., \"s3://bucket/prefix/\")</p> <p> TYPE: <code>str</code> </p> <code>aws_region</code> <p>AWS region name (e.g., \"us-east-1\")</p> <p> TYPE: <code>str</code> </p> <code>aws_access_key</code> <p>AWS access key ID</p> <p> TYPE: <code>str</code> </p> <code>aws_secret_key</code> <p>AWS secret access key</p> <p> TYPE: <code>str</code> </p> <code>dynamodb_locking_table</code> <p>Optional name of DynamoDB table to use as locking provider.                     If not provided, unsafe renames will be enabled.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> Notes <ul> <li>For production use with concurrent writes, it's strongly recommended to provide   a DynamoDB table for locking to ensure ACID guarantees.</li> <li>The DynamoDB table must have the following schema:</li> <li>Partition key: 'tablePath' (String)</li> <li>Sort key: 'fileName' (String)</li> <li>If no dynamodb_locking_table is provided, the S3LakeManager will use unsafe   renames which doesn't guarantee data consistency with concurrent writes.</li> <li>You can create the required DynamoDB table with:   <pre><code>aws dynamodb create-table\n    --table-name delta_log\n    --attribute-definitions AttributeName=tablePath,AttributeType=S AttributeName=fileName,AttributeType=S\n    --key-schema AttributeName=tablePath,KeyType=HASH AttributeName=fileName,KeyType=RANGE\n    --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5\n</code></pre></li> <li>Consider setting a TTL on the DynamoDB table to avoid it growing indefinitely.</li> </ul> Example <pre><code>from pdldb import S3LakeManager\n\n# Using unsafe renames (not safe for concurrent writes)\nlake_manager = S3LakeManager(\n    \"s3://mybucket/mydatalake/\",\n    aws_region=\"us-east-1\",\n    aws_access_key=\"YOUR_ACCESS_KEY\",\n    aws_secret_key=\"YOUR_SECRET_KEY\"\n)\n\n# Using DynamoDB locking provider (safe for concurrent writes)\nlake_manager = S3LakeManager(\n    \"s3://mybucket/mydatalake/\",\n    aws_region=\"us-east-1\",\n    aws_access_key=\"YOUR_ACCESS_KEY\",\n    aws_secret_key=\"YOUR_SECRET_KEY\",\n    dynamodb_locking_table=\"delta_log\"\n)\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def __init__(\n    self,\n    base_path: str,\n    aws_region: str,\n    aws_access_key: str,\n    aws_secret_key: str,\n    dynamodb_locking_table: Optional[str] = None,\n):\n    \"\"\"\n    Initialize a new S3LakeManager.\n\n    Args:\n        base_path: The S3 bucket path where the data lake will be stored (e.g., \"s3://bucket/prefix/\")\n        aws_region: AWS region name (e.g., \"us-east-1\")\n        aws_access_key: AWS access key ID\n        aws_secret_key: AWS secret access key\n        dynamodb_locking_table: Optional name of DynamoDB table to use as locking provider.\n                                If not provided, unsafe renames will be enabled.\n\n    Notes:\n        - For production use with concurrent writes, it's strongly recommended to provide\n          a DynamoDB table for locking to ensure ACID guarantees.\n        - The DynamoDB table must have the following schema:\n          - Partition key: 'tablePath' (String)\n          - Sort key: 'fileName' (String)\n        - If no dynamodb_locking_table is provided, the S3LakeManager will use unsafe\n          renames which doesn't guarantee data consistency with concurrent writes.\n        - You can create the required DynamoDB table with:\n          ```console\n          aws dynamodb create-table\n              --table-name delta_log\n              --attribute-definitions AttributeName=tablePath,AttributeType=S AttributeName=fileName,AttributeType=S\n              --key-schema AttributeName=tablePath,KeyType=HASH AttributeName=fileName,KeyType=RANGE\n              --provisioned-throughput ReadCapacityUnits=5,WriteCapacityUnits=5\n          ```\n        - Consider setting a TTL on the DynamoDB table to avoid it growing indefinitely.\n\n    Example:\n        ```python\n        from pdldb import S3LakeManager\n\n        # Using unsafe renames (not safe for concurrent writes)\n        lake_manager = S3LakeManager(\n            \"s3://mybucket/mydatalake/\",\n            aws_region=\"us-east-1\",\n            aws_access_key=\"YOUR_ACCESS_KEY\",\n            aws_secret_key=\"YOUR_SECRET_KEY\"\n        )\n\n        # Using DynamoDB locking provider (safe for concurrent writes)\n        lake_manager = S3LakeManager(\n            \"s3://mybucket/mydatalake/\",\n            aws_region=\"us-east-1\",\n            aws_access_key=\"YOUR_ACCESS_KEY\",\n            aws_secret_key=\"YOUR_SECRET_KEY\",\n            dynamodb_locking_table=\"delta_log\"\n        )\n        ```\n    \"\"\"\n    storage_options = {\n        \"AWS_REGION\": aws_region,\n        \"AWS_ACCESS_KEY_ID\": aws_access_key,\n        \"AWS_SECRET_ACCESS_KEY\": aws_secret_key,\n    }\n\n    if dynamodb_locking_table:\n        storage_options[\"AWS_S3_LOCKING_PROVIDER\"] = \"dynamodb\"\n        storage_options[\"DELTA_DYNAMO_TABLE_NAME\"] = dynamodb_locking_table\n    else:\n        storage_options[\"AWS_S3_ALLOW_UNSAFE_RENAME\"] = \"true\"\n\n    params = LakeManagerInitModel(\n        base_path=base_path, storage_options=storage_options\n    )\n    super().__init__(params.base_path, params.storage_options)\n    self.table_manager = S3TableManager(str(self.base_path), self.storage_options)\n</code></pre>"}]}