{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"pdldb","text":"<p>A high-performance analytical data store combining Polars' processing speed with Delta Lake's ACID transactions. This lightweight wrapper provides a database-like experience for local data processing.</p>"},{"location":"#overview","title":"Overview","text":"<p>pdldb creates a columnar data store that offers:</p> <ul> <li>The speed and efficiency of Polars for data operations</li> <li>The reliability and versioning of Delta Lake for data integrity</li> <li>Simple database-like operations with table management</li> <li>Flexible storage options with both local and cloud-based implementations</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>import polars as pl\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom pdldb import LocalLakeManager\n\n\n# 1. Generate sample data\nn_rows = int(10000)\nnp.random.seed(7)\nstart_date = datetime(2025, 3, 23)\n\ndata = {\n    \"sequence\": np.arange(1, n_rows + 1, dtype=np.int32),\n    \"id\": np.random.permutation(n_rows) + 1,\n    \"value_1\": np.random.normal(100, 15, n_rows).astype(np.float32),\n    \"value_2\": np.random.uniform(0, 1000, n_rows).astype(np.float32),\n    \"value_3\": np.random.choice([\"A\", \"B\", \"C\", \"D\"], n_rows),\n    \"value_4\": np.random.exponential(50, n_rows).astype(np.float32),\n    \"value_5\": [(start_date + timedelta(seconds=i)) for i in range(n_rows)],\n}\n\ndf = pl.DataFrame(data)\n\n# Split data for demonstration purposes\nfirst_half = df.slice(0, 6000)\nsecond_half = df.slice(5000, 5000)\n\n# 2. Initialize Delta Lake\nlake = LocalLakeManager(\"pdldb_demo\")\n\n# 3. Define schema\nschema = {\n    \"sequence\": pl.Int32,\n    \"id\": pl.Int64,\n    \"value_1\": pl.Float32,\n    \"value_2\": pl.Float32,\n    \"value_3\": pl.Utf8,\n    \"value_4\": pl.Float32,\n    \"value_5\": pl.Datetime(\"ns\"),\n}\n\n# 4. Create table and load initial data\nlake.create_table(\"my_table\", schema, primary_keys=[\"sequence\", \"value_5\"])\nlake.append_table(\"my_table\", first_half)\n\n# 5. Read data from table\ninitial_data = lake.get_data_frame(\"my_table\")\n\n# 6. Merge more data (with some overlapping records)\nlake.merge_table(\"my_table\", second_half, merge_condition=\"insert\")\nmerged_data = lake.get_data_frame(\"my_table\")\n\n# 7. Query full table and with SQL and lazy evaluation\nldf = lake.get_lazy_frame(\"my_table\")\nresult = ldf.sql(\"SELECT sequence, id FROM self WHERE sequence &gt; 100\").collect()\n\n# 8. Get table metadata\ntables = lake.list_tables()\ntable_info = lake.get_table_info(\"my_table\")\nschema = lake.get_table_schema(\"my_table\")\n\n# 9. Maintenance operations\nlake.optimize_table(\"my_table\")\nlake.vacuum_table(\"my_table\")\n\n# 10. Overwrite table\nsmall_df = df.slice(0, 50)\nlake.overwrite_table(\"my_table\", small_df)\nafter_overwrite = lake.get_data_frame(\"my_table\")\n\n# 11. Delete table\nlake.delete_table(\"my_table\")\n</code></pre>"},{"location":"api/api-reference/","title":"API Reference","text":"<p>This page provides auto-generated API documentation from docstrings.</p>"},{"location":"api/api-reference/#lake-managers","title":"Lake Managers","text":"<p>Core components for managing Delta Lake tables.</p>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager","title":"<code>pdldb.lake_manager.LakeManager</code>","text":"<p>Base class for managing a data lake with tables stored in Delta format.</p> <p>This class provides the foundation for creating, reading, updating, and managing Delta tables in a data lake. It's designed to be extended by specific implementations like LocalLakeManager.</p> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>class LakeManager:\n    \"\"\"\n    Base class for managing a data lake with tables stored in Delta format.\n\n    This class provides the foundation for creating, reading, updating, and managing\n    Delta tables in a data lake. It's designed to be extended by specific implementations\n    like LocalLakeManager.\n    \"\"\"\n\n    def __init__(\n        self, base_path: str, storage_options: Optional[Dict[str, Any]] = None\n    ):\n        \"\"\"\n        Initialize a new LakeManager.\n\n        Args:\n            base_path: The base path where the data lake will be stored\n            storage_options: Optional cloud storage-specific parameters\n        \"\"\"\n        params = LakeManagerInitModel(\n            base_path=base_path, storage_options=storage_options\n        )\n        self.base_path = Path(params.base_path)\n        self.storage_options = params.storage_options\n        self.table_manager = None\n\n    def _check_table_exists(self, table_name: str) -&gt; None:\n        if table_name not in self.table_manager.tables:\n            raise ValueError(f\"Table {table_name} does not exist\")\n\n    def _check_table_not_exists(self, table_name: str) -&gt; None:\n        if table_name in self.table_manager.tables:\n            raise ValueError(f\"Table {table_name} already exists\")\n\n    def create_table(\n        self,\n        table_name: str,\n        table_schema: Dict[str, Any],\n        primary_keys: Union[str, List[str]],\n    ) -&gt; None:\n        \"\"\"\n        Create a new table in the data lake.\n\n        Args:\n            table_name: Name of the table to create\n            table_schema: Schema definition for the new table\n            primary_keys: Primary key column(s) for the table\n\n        Example: Single primary key\n            ```python\n            from pdldb import LocalLakeManager\n            import polars as pl\n\n            lake_manager = LocalLakeManager(\"data\")\n            schema = {\n                \"sequence\": pl.Int32,\n                \"value_1\": pl.Float64,\n                \"value_2\": pl.Utf8,\n                \"value_3\": pl.Float64,\n                \"value_4\": pl.Float64,\n                \"value_5\": pl.Datetime(\"ns\"),\n            }\n            primary_keys = \"sequence\"\n            lake_manager.create_table(\"my_table\", schema, primary_keys)\n            ```\n\n        Example: Composite primary key\n            ```python\n            primary_keys = [\"sequence\", \"value_1\"]\n            lake_manager.create_table(\"my_table\", schema, primary_keys)\n            ```\n        \"\"\"\n        params = TableCreateModel(\n            table_name=table_name, table_schema=table_schema, primary_keys=primary_keys\n        )\n\n        self._check_table_not_exists(table_name=params.table_name)\n        self.table_manager.create_table(\n            table_name=params.table_name,\n            table_schema=params.table_schema,\n            primary_keys=params.primary_keys,\n        )\n\n    def append_table(\n        self,\n        table_name: str,\n        df: pl.DataFrame,\n        delta_write_options: Optional[Dict[str, Any]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Append data to an existing table.\n\n        Args:\n            table_name: Name of the table to append to\n            df: DataFrame containing the data to append\n            delta_write_options: Optional configuration for the delta write operation\n\n        Example:\n            ```python\n            lake_manager.append_table(\"my_table\", newdata)\n            ```\n        \"\"\"\n        params = TableOperationModel(\n            table_name=table_name, df=df, delta_write_options=delta_write_options\n        )\n\n        self._check_table_exists(table_name=params.table_name)\n        self.table_manager.append(\n            table_name=params.table_name,\n            df=params.df,\n            delta_write_options=params.delta_write_options,\n        )\n\n    def merge_table(\n        self,\n        table_name: str,\n        df: pl.DataFrame,\n        merge_condition: str = \"insert\",\n        delta_write_options: Optional[Dict[str, Any]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Merge data into an existing table based on the specified merge condition.\n\n        Args:\n            table_name: Name of the table to merge data into\n            df: DataFrame containing the data to merge\n            merge_condition: Type of merge operation to perform (update, insert, delete, upsert, upsert_delete)\n            delta_write_options: Optional configuration for the delta write operation\n\n        merge_condition:\n            - update: Update existing rows only from the new data\n            - insert: Insert new rows only from the new data\n            - delete: Delete existing rows that exist in the new data\n            - upsert: Update existing rows and insert new rows from the new data\n            - upsert_delete: Update existing rows, insert new rows, and delete rows that don't exist in the new data\n\n        Example:\n            ```python\n            lake_manager.merge_table(\"my_table\", new_data, merge_condition=\"upsert\")\n            ```\n        \"\"\"\n        params = MergeOperationModel(\n            table_name=table_name,\n            df=df,\n            merge_condition=merge_condition,\n            delta_write_options=delta_write_options,\n        )\n\n        self._check_table_exists(table_name=params.table_name)\n\n        self.table_manager.merge(\n            table_name=params.table_name,\n            df=params.df,\n            delta_write_options=params.delta_write_options,\n            merge_condition=params.merge_condition,\n        )\n\n    def overwrite_table(\n        self,\n        table_name: str,\n        df: pl.DataFrame,\n        delta_write_options: Optional[Dict[str, Any]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Overwrite an existing table with new data.\n\n        Args:\n            table_name: Name of the table to overwrite\n            df: DataFrame containing the new data\n            delta_write_options: Optional configuration for the delta write operation\n\n        Example:\n            ```python\n            lake_manager.overwrite_table(\"my_table\", new_data)\n            ```\n        \"\"\"\n        params = TableOperationModel(\n            table_name=table_name, df=df, delta_write_options=delta_write_options\n        )\n\n        self._check_table_exists(table_name=params.table_name)\n        self.table_manager.overwrite(\n            table_name=params.table_name,\n            df=params.df,\n            delta_write_options=params.delta_write_options,\n        )\n\n    def get_data_frame(self, table_name: str) -&gt; pl.DataFrame:\n        \"\"\"\n        Get an eager DataFrame from a table.\n\n        Args:\n            table_name: Name of the table to read\n\n        Returns:\n            A Polars DataFrame containing the table data\n\n        Example:\n            ```python\n            df = lake_manager.get_data_frame(\"my_table\")\n            ```\n        \"\"\"\n        params = TableNameModel(table_name=table_name)\n        self._check_table_exists(table_name=params.table_name)\n        return self.table_manager.get_data_frame(table_name=params.table_name)\n\n    def get_lazy_frame(self, table_name: str) -&gt; pl.LazyFrame:\n        \"\"\"\n        Get a lazy DataFrame from a table for deferred execution.\n\n        Args:\n            table_name: Name of the table to read\n\n        Returns:\n            A Polars LazyFrame referencing the table data\n\n        Example:\n            ```python\n            lazy_frame = lake_manager.get_lazy_frame(\"my_table\")\n            result = lazy_frame.filter(col(\"column\") &gt; 10).select([\"column\"])\n            result.collect()\n            ```\n        \"\"\"\n        params = TableNameModel(table_name=table_name)\n        self._check_table_exists(table_name=params.table_name)\n        return self.table_manager.get_lazy_frame(table_name=params.table_name)\n\n    def optimize_table(\n        self,\n        table_name: str,\n        target_size: int = 512 * 1024 * 1024,\n        max_concurrent_tasks: Optional[int] = None,\n        writer_properties: Optional[Dict[str, Any]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Optimize a table by compacting small files in to files of the target size.\n        Optimizing a table can improve query performance and cloud costs.\n\n        Args:\n            table_name: Name of the table to optimize\n            target_size: Target file size in bytes for optimization\n            max_concurrent_tasks: Maximum number of concurrent tasks for optimization\n            writer_properties: Optional writer properties for optimization\n\n        Example:\n            ```python\n            lake_manager.optimize_table(\"my_table\", target_size=512*1024*1024)\n            ```\n        \"\"\"\n        params = OptimizeTableModel(\n            table_name=table_name,\n            target_size=target_size,\n            max_concurrent_tasks=max_concurrent_tasks,\n            writer_properties=writer_properties,\n        )\n\n        self._check_table_exists(table_name=params.table_name)\n        self.table_manager.optimize_table(\n            table_name=params.table_name,\n            target_size=params.target_size,\n            max_concurrent_tasks=params.max_concurrent_tasks,\n            writer_properties=params.writer_properties,\n        )\n\n    def vacuum_table(\n        self,\n        table_name: str,\n        retention_hours: Optional[int] = 0,\n        enforce_retention_duration: Optional[bool] = False,\n    ) -&gt; None:\n        \"\"\"\n        Clean up old data files from a table based on the retention period.\n        Old data files are those that are no longer referenced by the table.\n\n        Args:\n            table_name: Name of the table to vacuum\n            retention_hours: Retention period in hours (0 means delete all unreferenced files)\n            enforce_retention_duration: Whether to enforce the retention period\n\n        Example:\n            ```python\n            lake_manager.vacuum_table(\"my_table\", retention_hours=24)\n            ```\n        \"\"\"\n        params = VacuumTableModel(\n            table_name=table_name,\n            retention_hours=retention_hours,\n            enforce_retention_duration=enforce_retention_duration,\n        )\n\n        self._check_table_exists(table_name=params.table_name)\n        self.table_manager.vacuum_table(\n            table_name=params.table_name,\n            retention_hours=params.retention_hours,\n            enforce_retention_duration=params.enforce_retention_duration,\n        )\n\n    def list_tables(self) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"\n        List all tables in the data lake.\n\n        Returns:\n            A dictionary mapping table names to their metadata\n\n        Example:\n            ```python\n            lake_manager.list_tables()\n            ```\n        \"\"\"\n        return self.table_manager.list_tables()\n\n    def get_table_info(self, table_name: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get detailed information about a specific table.\n\n        Args:\n            table_name: Name of the table to get information for\n\n        Returns:\n            A dictionary containing detailed table information\n\n        Example:\n            ```python\n            lake_manager.get_table_info(\"my_table\")\n            ```\n        \"\"\"\n        params = TableNameModel(table_name=table_name)\n        self._check_table_exists(table_name=params.table_name)\n        return self.table_manager.get_table_info(table_name=params.table_name)\n\n    def get_table_schema(self, table_name: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get the schema definition for a specific table.\n\n        Args:\n            table_name: Name of the table to get the schema for\n\n        Returns:\n            A dictionary representing the table schema\n\n        Example:\n            ```python\n            lake_manager.get_table_schema(\"my_table\")\n            ```\n        \"\"\"\n        params = TableNameModel(table_name=table_name)\n        self._check_table_exists(table_name=params.table_name)\n        return self.table_manager.get_table_schema(table_name=params.table_name)\n\n    def delete_table(self, table_name: str) -&gt; bool:\n        \"\"\"\n        Delete a table from the data lake.\n        Deleted data files are not recoverable, so use with caution.\n\n        Args:\n            table_name: Name of the table to delete\n\n        Returns:\n            True if the table was successfully deleted\n\n        Example:\n            ```python\n            lake_manager.delete_table(\"my_table\")\n            ```\n        \"\"\"\n        params = TableNameModel(table_name=table_name)\n        self._check_table_exists(table_name=params.table_name)\n        return self.table_manager.delete_table(table_name=params.table_name)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        pass\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager-functions","title":"Functions","text":""},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.__init__","title":"<code>__init__(base_path, storage_options=None)</code>","text":"<p>Initialize a new LakeManager.</p> PARAMETER DESCRIPTION <code>base_path</code> <p>The base path where the data lake will be stored</p> <p> TYPE: <code>str</code> </p> <code>storage_options</code> <p>Optional cloud storage-specific parameters</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def __init__(\n    self, base_path: str, storage_options: Optional[Dict[str, Any]] = None\n):\n    \"\"\"\n    Initialize a new LakeManager.\n\n    Args:\n        base_path: The base path where the data lake will be stored\n        storage_options: Optional cloud storage-specific parameters\n    \"\"\"\n    params = LakeManagerInitModel(\n        base_path=base_path, storage_options=storage_options\n    )\n    self.base_path = Path(params.base_path)\n    self.storage_options = params.storage_options\n    self.table_manager = None\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.append_table","title":"<code>append_table(table_name, df, delta_write_options=None)</code>","text":"<p>Append data to an existing table.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to append to</p> <p> TYPE: <code>str</code> </p> <code>df</code> <p>DataFrame containing the data to append</p> <p> TYPE: <code>DataFrame</code> </p> <code>delta_write_options</code> <p>Optional configuration for the delta write operation</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> Example <pre><code>lake_manager.append_table(\"my_table\", newdata)\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def append_table(\n    self,\n    table_name: str,\n    df: pl.DataFrame,\n    delta_write_options: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"\n    Append data to an existing table.\n\n    Args:\n        table_name: Name of the table to append to\n        df: DataFrame containing the data to append\n        delta_write_options: Optional configuration for the delta write operation\n\n    Example:\n        ```python\n        lake_manager.append_table(\"my_table\", newdata)\n        ```\n    \"\"\"\n    params = TableOperationModel(\n        table_name=table_name, df=df, delta_write_options=delta_write_options\n    )\n\n    self._check_table_exists(table_name=params.table_name)\n    self.table_manager.append(\n        table_name=params.table_name,\n        df=params.df,\n        delta_write_options=params.delta_write_options,\n    )\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.create_table","title":"<code>create_table(table_name, table_schema, primary_keys)</code>","text":"<p>Create a new table in the data lake.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to create</p> <p> TYPE: <code>str</code> </p> <code>table_schema</code> <p>Schema definition for the new table</p> <p> TYPE: <code>Dict[str, Any]</code> </p> <code>primary_keys</code> <p>Primary key column(s) for the table</p> <p> TYPE: <code>Union[str, List[str]]</code> </p> Single primary key <pre><code>from pdldb import LocalLakeManager\nimport polars as pl\n\nlake_manager = LocalLakeManager(\"data\")\nschema = {\n    \"sequence\": pl.Int32,\n    \"value_1\": pl.Float64,\n    \"value_2\": pl.Utf8,\n    \"value_3\": pl.Float64,\n    \"value_4\": pl.Float64,\n    \"value_5\": pl.Datetime(\"ns\"),\n}\nprimary_keys = \"sequence\"\nlake_manager.create_table(\"my_table\", schema, primary_keys)\n</code></pre> Composite primary key <pre><code>primary_keys = [\"sequence\", \"value_1\"]\nlake_manager.create_table(\"my_table\", schema, primary_keys)\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def create_table(\n    self,\n    table_name: str,\n    table_schema: Dict[str, Any],\n    primary_keys: Union[str, List[str]],\n) -&gt; None:\n    \"\"\"\n    Create a new table in the data lake.\n\n    Args:\n        table_name: Name of the table to create\n        table_schema: Schema definition for the new table\n        primary_keys: Primary key column(s) for the table\n\n    Example: Single primary key\n        ```python\n        from pdldb import LocalLakeManager\n        import polars as pl\n\n        lake_manager = LocalLakeManager(\"data\")\n        schema = {\n            \"sequence\": pl.Int32,\n            \"value_1\": pl.Float64,\n            \"value_2\": pl.Utf8,\n            \"value_3\": pl.Float64,\n            \"value_4\": pl.Float64,\n            \"value_5\": pl.Datetime(\"ns\"),\n        }\n        primary_keys = \"sequence\"\n        lake_manager.create_table(\"my_table\", schema, primary_keys)\n        ```\n\n    Example: Composite primary key\n        ```python\n        primary_keys = [\"sequence\", \"value_1\"]\n        lake_manager.create_table(\"my_table\", schema, primary_keys)\n        ```\n    \"\"\"\n    params = TableCreateModel(\n        table_name=table_name, table_schema=table_schema, primary_keys=primary_keys\n    )\n\n    self._check_table_not_exists(table_name=params.table_name)\n    self.table_manager.create_table(\n        table_name=params.table_name,\n        table_schema=params.table_schema,\n        primary_keys=params.primary_keys,\n    )\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.delete_table","title":"<code>delete_table(table_name)</code>","text":"<p>Delete a table from the data lake. Deleted data files are not recoverable, so use with caution.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to delete</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if the table was successfully deleted</p> Example <pre><code>lake_manager.delete_table(\"my_table\")\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def delete_table(self, table_name: str) -&gt; bool:\n    \"\"\"\n    Delete a table from the data lake.\n    Deleted data files are not recoverable, so use with caution.\n\n    Args:\n        table_name: Name of the table to delete\n\n    Returns:\n        True if the table was successfully deleted\n\n    Example:\n        ```python\n        lake_manager.delete_table(\"my_table\")\n        ```\n    \"\"\"\n    params = TableNameModel(table_name=table_name)\n    self._check_table_exists(table_name=params.table_name)\n    return self.table_manager.delete_table(table_name=params.table_name)\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.get_data_frame","title":"<code>get_data_frame(table_name)</code>","text":"<p>Get an eager DataFrame from a table.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to read</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>DataFrame</code> <p>A Polars DataFrame containing the table data</p> Example <pre><code>df = lake_manager.get_data_frame(\"my_table\")\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def get_data_frame(self, table_name: str) -&gt; pl.DataFrame:\n    \"\"\"\n    Get an eager DataFrame from a table.\n\n    Args:\n        table_name: Name of the table to read\n\n    Returns:\n        A Polars DataFrame containing the table data\n\n    Example:\n        ```python\n        df = lake_manager.get_data_frame(\"my_table\")\n        ```\n    \"\"\"\n    params = TableNameModel(table_name=table_name)\n    self._check_table_exists(table_name=params.table_name)\n    return self.table_manager.get_data_frame(table_name=params.table_name)\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.get_lazy_frame","title":"<code>get_lazy_frame(table_name)</code>","text":"<p>Get a lazy DataFrame from a table for deferred execution.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to read</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>LazyFrame</code> <p>A Polars LazyFrame referencing the table data</p> Example <pre><code>lazy_frame = lake_manager.get_lazy_frame(\"my_table\")\nresult = lazy_frame.filter(col(\"column\") &gt; 10).select([\"column\"])\nresult.collect()\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def get_lazy_frame(self, table_name: str) -&gt; pl.LazyFrame:\n    \"\"\"\n    Get a lazy DataFrame from a table for deferred execution.\n\n    Args:\n        table_name: Name of the table to read\n\n    Returns:\n        A Polars LazyFrame referencing the table data\n\n    Example:\n        ```python\n        lazy_frame = lake_manager.get_lazy_frame(\"my_table\")\n        result = lazy_frame.filter(col(\"column\") &gt; 10).select([\"column\"])\n        result.collect()\n        ```\n    \"\"\"\n    params = TableNameModel(table_name=table_name)\n    self._check_table_exists(table_name=params.table_name)\n    return self.table_manager.get_lazy_frame(table_name=params.table_name)\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.get_table_info","title":"<code>get_table_info(table_name)</code>","text":"<p>Get detailed information about a specific table.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to get information for</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code> <p>A dictionary containing detailed table information</p> Example <pre><code>lake_manager.get_table_info(\"my_table\")\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def get_table_info(self, table_name: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get detailed information about a specific table.\n\n    Args:\n        table_name: Name of the table to get information for\n\n    Returns:\n        A dictionary containing detailed table information\n\n    Example:\n        ```python\n        lake_manager.get_table_info(\"my_table\")\n        ```\n    \"\"\"\n    params = TableNameModel(table_name=table_name)\n    self._check_table_exists(table_name=params.table_name)\n    return self.table_manager.get_table_info(table_name=params.table_name)\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.get_table_schema","title":"<code>get_table_schema(table_name)</code>","text":"<p>Get the schema definition for a specific table.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to get the schema for</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Dict[str, Any]</code> <p>A dictionary representing the table schema</p> Example <pre><code>lake_manager.get_table_schema(\"my_table\")\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def get_table_schema(self, table_name: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get the schema definition for a specific table.\n\n    Args:\n        table_name: Name of the table to get the schema for\n\n    Returns:\n        A dictionary representing the table schema\n\n    Example:\n        ```python\n        lake_manager.get_table_schema(\"my_table\")\n        ```\n    \"\"\"\n    params = TableNameModel(table_name=table_name)\n    self._check_table_exists(table_name=params.table_name)\n    return self.table_manager.get_table_schema(table_name=params.table_name)\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.list_tables","title":"<code>list_tables()</code>","text":"<p>List all tables in the data lake.</p> RETURNS DESCRIPTION <code>Dict[str, Dict[str, Any]]</code> <p>A dictionary mapping table names to their metadata</p> Example <pre><code>lake_manager.list_tables()\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def list_tables(self) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"\n    List all tables in the data lake.\n\n    Returns:\n        A dictionary mapping table names to their metadata\n\n    Example:\n        ```python\n        lake_manager.list_tables()\n        ```\n    \"\"\"\n    return self.table_manager.list_tables()\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.merge_table","title":"<code>merge_table(table_name, df, merge_condition='insert', delta_write_options=None)</code>","text":"<p>Merge data into an existing table based on the specified merge condition.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to merge data into</p> <p> TYPE: <code>str</code> </p> <code>df</code> <p>DataFrame containing the data to merge</p> <p> TYPE: <code>DataFrame</code> </p> <code>merge_condition</code> <p>Type of merge operation to perform (update, insert, delete, upsert, upsert_delete)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'insert'</code> </p> <code>delta_write_options</code> <p>Optional configuration for the delta write operation</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> merge_condition <ul> <li>update: Update existing rows only from the new data</li> <li>insert: Insert new rows only from the new data</li> <li>delete: Delete existing rows that exist in the new data</li> <li>upsert: Update existing rows and insert new rows from the new data</li> <li>upsert_delete: Update existing rows, insert new rows, and delete rows that don't exist in the new data</li> </ul> Example <pre><code>lake_manager.merge_table(\"my_table\", new_data, merge_condition=\"upsert\")\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def merge_table(\n    self,\n    table_name: str,\n    df: pl.DataFrame,\n    merge_condition: str = \"insert\",\n    delta_write_options: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"\n    Merge data into an existing table based on the specified merge condition.\n\n    Args:\n        table_name: Name of the table to merge data into\n        df: DataFrame containing the data to merge\n        merge_condition: Type of merge operation to perform (update, insert, delete, upsert, upsert_delete)\n        delta_write_options: Optional configuration for the delta write operation\n\n    merge_condition:\n        - update: Update existing rows only from the new data\n        - insert: Insert new rows only from the new data\n        - delete: Delete existing rows that exist in the new data\n        - upsert: Update existing rows and insert new rows from the new data\n        - upsert_delete: Update existing rows, insert new rows, and delete rows that don't exist in the new data\n\n    Example:\n        ```python\n        lake_manager.merge_table(\"my_table\", new_data, merge_condition=\"upsert\")\n        ```\n    \"\"\"\n    params = MergeOperationModel(\n        table_name=table_name,\n        df=df,\n        merge_condition=merge_condition,\n        delta_write_options=delta_write_options,\n    )\n\n    self._check_table_exists(table_name=params.table_name)\n\n    self.table_manager.merge(\n        table_name=params.table_name,\n        df=params.df,\n        delta_write_options=params.delta_write_options,\n        merge_condition=params.merge_condition,\n    )\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.optimize_table","title":"<code>optimize_table(table_name, target_size=512 * 1024 * 1024, max_concurrent_tasks=None, writer_properties=None)</code>","text":"<p>Optimize a table by compacting small files in to files of the target size. Optimizing a table can improve query performance and cloud costs.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to optimize</p> <p> TYPE: <code>str</code> </p> <code>target_size</code> <p>Target file size in bytes for optimization</p> <p> TYPE: <code>int</code> DEFAULT: <code>512 * 1024 * 1024</code> </p> <code>max_concurrent_tasks</code> <p>Maximum number of concurrent tasks for optimization</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <code>writer_properties</code> <p>Optional writer properties for optimization</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> Example <pre><code>lake_manager.optimize_table(\"my_table\", target_size=512*1024*1024)\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def optimize_table(\n    self,\n    table_name: str,\n    target_size: int = 512 * 1024 * 1024,\n    max_concurrent_tasks: Optional[int] = None,\n    writer_properties: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"\n    Optimize a table by compacting small files in to files of the target size.\n    Optimizing a table can improve query performance and cloud costs.\n\n    Args:\n        table_name: Name of the table to optimize\n        target_size: Target file size in bytes for optimization\n        max_concurrent_tasks: Maximum number of concurrent tasks for optimization\n        writer_properties: Optional writer properties for optimization\n\n    Example:\n        ```python\n        lake_manager.optimize_table(\"my_table\", target_size=512*1024*1024)\n        ```\n    \"\"\"\n    params = OptimizeTableModel(\n        table_name=table_name,\n        target_size=target_size,\n        max_concurrent_tasks=max_concurrent_tasks,\n        writer_properties=writer_properties,\n    )\n\n    self._check_table_exists(table_name=params.table_name)\n    self.table_manager.optimize_table(\n        table_name=params.table_name,\n        target_size=params.target_size,\n        max_concurrent_tasks=params.max_concurrent_tasks,\n        writer_properties=params.writer_properties,\n    )\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.overwrite_table","title":"<code>overwrite_table(table_name, df, delta_write_options=None)</code>","text":"<p>Overwrite an existing table with new data.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to overwrite</p> <p> TYPE: <code>str</code> </p> <code>df</code> <p>DataFrame containing the new data</p> <p> TYPE: <code>DataFrame</code> </p> <code>delta_write_options</code> <p>Optional configuration for the delta write operation</p> <p> TYPE: <code>Optional[Dict[str, Any]]</code> DEFAULT: <code>None</code> </p> Example <pre><code>lake_manager.overwrite_table(\"my_table\", new_data)\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def overwrite_table(\n    self,\n    table_name: str,\n    df: pl.DataFrame,\n    delta_write_options: Optional[Dict[str, Any]] = None,\n) -&gt; None:\n    \"\"\"\n    Overwrite an existing table with new data.\n\n    Args:\n        table_name: Name of the table to overwrite\n        df: DataFrame containing the new data\n        delta_write_options: Optional configuration for the delta write operation\n\n    Example:\n        ```python\n        lake_manager.overwrite_table(\"my_table\", new_data)\n        ```\n    \"\"\"\n    params = TableOperationModel(\n        table_name=table_name, df=df, delta_write_options=delta_write_options\n    )\n\n    self._check_table_exists(table_name=params.table_name)\n    self.table_manager.overwrite(\n        table_name=params.table_name,\n        df=params.df,\n        delta_write_options=params.delta_write_options,\n    )\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LakeManager.vacuum_table","title":"<code>vacuum_table(table_name, retention_hours=0, enforce_retention_duration=False)</code>","text":"<p>Clean up old data files from a table based on the retention period. Old data files are those that are no longer referenced by the table.</p> PARAMETER DESCRIPTION <code>table_name</code> <p>Name of the table to vacuum</p> <p> TYPE: <code>str</code> </p> <code>retention_hours</code> <p>Retention period in hours (0 means delete all unreferenced files)</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>0</code> </p> <code>enforce_retention_duration</code> <p>Whether to enforce the retention period</p> <p> TYPE: <code>Optional[bool]</code> DEFAULT: <code>False</code> </p> Example <pre><code>lake_manager.vacuum_table(\"my_table\", retention_hours=24)\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def vacuum_table(\n    self,\n    table_name: str,\n    retention_hours: Optional[int] = 0,\n    enforce_retention_duration: Optional[bool] = False,\n) -&gt; None:\n    \"\"\"\n    Clean up old data files from a table based on the retention period.\n    Old data files are those that are no longer referenced by the table.\n\n    Args:\n        table_name: Name of the table to vacuum\n        retention_hours: Retention period in hours (0 means delete all unreferenced files)\n        enforce_retention_duration: Whether to enforce the retention period\n\n    Example:\n        ```python\n        lake_manager.vacuum_table(\"my_table\", retention_hours=24)\n        ```\n    \"\"\"\n    params = VacuumTableModel(\n        table_name=table_name,\n        retention_hours=retention_hours,\n        enforce_retention_duration=enforce_retention_duration,\n    )\n\n    self._check_table_exists(table_name=params.table_name)\n    self.table_manager.vacuum_table(\n        table_name=params.table_name,\n        retention_hours=params.retention_hours,\n        enforce_retention_duration=params.enforce_retention_duration,\n    )\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LocalLakeManager","title":"<code>pdldb.lake_manager.LocalLakeManager</code>","text":"<p>               Bases: <code>LakeManager</code></p> <p>Implementation of LakeManager for local filesystem storage.</p> <p>This class extends the base LakeManager to provide specific functionality for managing Delta tables in a local filesystem.</p> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>class LocalLakeManager(LakeManager):\n    \"\"\"\n    Implementation of LakeManager for local filesystem storage.\n\n    This class extends the base LakeManager to provide specific functionality\n    for managing Delta tables in a local filesystem.\n    \"\"\"\n\n    def __init__(self, base_path: str):\n        \"\"\"\n        Initialize a new LocalLakeManager.\n\n        Args:\n            base_path: The local filesystem path where the data lake will be stored\n\n        Example:\n            ```python\n            from pdldb.lake_manager import LocalLakeManager\n            lake_manager = LocalLakeManager(\"data\")\n            ```\n        \"\"\"\n        params = LakeManagerInitModel(base_path=base_path, storage_options=None)\n        super().__init__(params.base_path, params.storage_options)\n        self.base_path.mkdir(parents=True, exist_ok=True)\n        self.table_manager = LocalTableManager(\n            str(self.base_path), self.storage_options\n        )\n</code></pre>"},{"location":"api/api-reference/#pdldb.lake_manager.LocalLakeManager-functions","title":"Functions","text":""},{"location":"api/api-reference/#pdldb.lake_manager.LocalLakeManager.__init__","title":"<code>__init__(base_path)</code>","text":"<p>Initialize a new LocalLakeManager.</p> PARAMETER DESCRIPTION <code>base_path</code> <p>The local filesystem path where the data lake will be stored</p> <p> TYPE: <code>str</code> </p> Example <pre><code>from pdldb.lake_manager import LocalLakeManager\nlake_manager = LocalLakeManager(\"data\")\n</code></pre> Source code in <code>src/pdldb/lake_manager.py</code> <pre><code>def __init__(self, base_path: str):\n    \"\"\"\n    Initialize a new LocalLakeManager.\n\n    Args:\n        base_path: The local filesystem path where the data lake will be stored\n\n    Example:\n        ```python\n        from pdldb.lake_manager import LocalLakeManager\n        lake_manager = LocalLakeManager(\"data\")\n        ```\n    \"\"\"\n    params = LakeManagerInitModel(base_path=base_path, storage_options=None)\n    super().__init__(params.base_path, params.storage_options)\n    self.base_path.mkdir(parents=True, exist_ok=True)\n    self.table_manager = LocalTableManager(\n        str(self.base_path), self.storage_options\n    )\n</code></pre>"},{"location":"api/api-reference/#backup-managers","title":"Backup Managers","text":"<p>Components for backing up and restoring Delta Lake data.</p>"},{"location":"api/api-reference/#pdldb.LocalBackupManager","title":"<code>pdldb.LocalBackupManager</code>","text":"<p>Manages local file backups with support for both full and mirror backup strategies.</p> <p>This class provides functionality to:</p> <ul> <li>Create full backups (complete tar.gz archives with manifests)</li> <li>Create/update mirror backups (individual files with change detection)</li> <li>Restore files from either backup type</li> <li>List available backups with their metadata</li> </ul> <p>All backups are stored in a structured directory hierarchy:</p> <ul> <li>{backup_directory}/{prefix}full_backups/ - Contains all full backups</li> <li>{backup_directory}/{prefix}mirror_backup/ - Contains the single mirror backup</li> </ul> <p>Each backup includes a manifest.json file with metadata about the backup, including file hashes, modification times, creation timestamp, and backup type.</p> Source code in <code>src/pdldb/local_backup_manager.py</code> <pre><code>class LocalBackupManager:\n    \"\"\"\n    Manages local file backups with support for both full and mirror backup strategies.\n\n    This class provides functionality to:\n\n    - Create full backups (complete tar.gz archives with manifests)\n    - Create/update mirror backups (individual files with change detection)\n    - Restore files from either backup type\n    - List available backups with their metadata\n\n    All backups are stored in a structured directory hierarchy:\n\n    - {backup_directory}/{prefix}full_backups/ - Contains all full backups\n    - {backup_directory}/{prefix}mirror_backup/ - Contains the single mirror backup\n\n    Each backup includes a manifest.json file with metadata about the backup, including file hashes, modification times, creation timestamp, and backup type.\n    \"\"\"\n    class Config(BaseModel):\n        backup_directory: DirectoryPath\n        prefix: str = \"pdldb_backups/\"\n\n    def __init__(\n        self, backup_directory: Union[str, os.PathLike], prefix: str = \"pdldb_backups/\"\n    ):\n        \"\"\"\n        Initialize a LocalBackupManager with the specified backup directory and prefix.\n\n        Args:\n            backup_directory: The base directory where all backups will be stored\n            prefix: Optional prefix for backup subdirectories (default: \"pdldb_backups/\")\n                Used to organize backups within the backup_directory\n\n        !!! note\n            This method creates the necessary subdirectories for both full and mirror \n            backups if they don't already exist.\n        \"\"\"\n        config = self.Config(\n            backup_directory=os.path.abspath(backup_directory), prefix=prefix\n        )\n        self.backup_directory = config.backup_directory\n        self.prefix = config.prefix\n        self.full_prefix = os.path.join(self.backup_directory, f\"{prefix}full_backups/\")\n        self.mir_prefix = os.path.join(self.backup_directory, f\"{prefix}mirror_backup/\")\n        os.makedirs(self.full_prefix, exist_ok=True)\n        os.makedirs(self.mir_prefix, exist_ok=True)\n\n    def _get_file_hash(self, filepath: Union[str, os.PathLike]) -&gt; Optional[str]:\n        if not os.path.isfile(filepath):\n            return None\n        hasher = hashlib.sha256()\n        with open(filepath, \"rb\") as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                hasher.update(chunk)\n        return hasher.hexdigest()\n\n    def _get_manifest_path(self, backup_name: str) -&gt; str:\n        return os.path.join(self.full_prefix, backup_name, \"manifest.json\")\n\n    def _get_mir_manifest_path(self) -&gt; str:\n        return os.path.join(self.mir_prefix, \"manifest.json\")\n\n    def _load_manifest(self, backup_name: Optional[str] = None) -&gt; ManifestModel:\n        if backup_name:\n            manifest_path = self._get_manifest_path(backup_name)\n        else:\n            manifest_path = self._get_mir_manifest_path()\n\n        if os.path.exists(manifest_path):\n            with open(manifest_path, \"r\") as f:\n                data = json.load(f)\n                return ManifestModel(**data)\n        return ManifestModel(files={}, created_at=datetime.now().isoformat())\n\n    def _save_manifest(\n        self, manifest: ManifestModel, backup_name: Optional[str] = None\n    ) -&gt; None:\n        if backup_name:\n            manifest_path = self._get_manifest_path(backup_name)\n            os.makedirs(os.path.dirname(manifest_path), exist_ok=True)\n        else:\n            manifest_path = self._get_mir_manifest_path()\n\n        with open(manifest_path, \"w\") as f:\n            json.dump(manifest.model_dump(), f, indent=2)\n\n    def full_backup(\n        self, source_path: Union[str, os.PathLike], backup_name: Optional[str] = None\n    ) -&gt; str:\n        \"\"\"\n        Creates a complete backup of a source directory as a compressed archive.\n\n        A full backup creates a new backup directory containing:\n\n        - A compressed tar.gz archive of the entire source directory\n        - A manifest.json file with metadata and file hashes for all files\n\n        Unlike mirror backups, each full backup is stored as a separate archive, allowing for multiple backup versions to be maintained.\n\n        Args:\n            source_path: Path to the directory that should be backed up\n            backup_name: Optional custom name for the backup. If not provided,\n                        a name will be generated using the source directory name\n                        and current timestamp (e.g., \"mydir_20250325_123045\")\n\n        Returns:\n            str: The name of the created backup (either the provided backup_name\n                or the auto-generated name)\n\n        !!! note\n            - If a backup with the specified name already exists, its contents will be overwritten.\n            - The manifest includes SHA-256 hashes and modification times for all files,\n            which can be used for verification or restoration purposes.\n\n        Example:\n            ```python\n            manager = LocalBackupManager(backup_directory=\"/path/to/backups/\")\n            backup_name = manager.full_backup(source_path=\"/path/to/source_dir/\")\n            print(backup_name)\n            # Output: \"source_dir_20250325_123045\"\n            ```\n        \"\"\"\n        params = FullBackupParams(source_path=source_path, backup_name=backup_name)\n\n        source_path = os.path.abspath(params.source_path)\n        source_dir = os.path.basename(source_path)\n\n        if not params.backup_name:\n            date_suffix = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            backup_name = f\"{source_dir}_{date_suffix}\"\n        else:\n            backup_name = params.backup_name\n\n        backup_dir = os.path.join(self.full_prefix, backup_name)\n        os.makedirs(backup_dir, exist_ok=True)\n        archive_path = os.path.join(backup_dir, \"full_backup.tar.gz\")\n\n        with tarfile.open(archive_path, \"w:gz\") as tar:\n            tar.add(source_path, arcname=os.path.basename(source_path))\n\n        manifest = ManifestModel(\n            files={}, created_at=datetime.now().isoformat(), type=\"full\"\n        )\n\n        for root, _, files in os.walk(source_path):\n            for file in files:\n                file_path = os.path.join(root, file)\n                rel_path = os.path.relpath(file_path, source_path)\n                manifest.files[rel_path] = FileInfo(\n                    hash=self._get_file_hash(file_path) or \"\",\n                    mtime=os.path.getmtime(file_path),\n                )\n\n        self._save_manifest(manifest, backup_name)\n        return backup_name\n\n    def mirror_backup(self, source_path: Union[str, os.PathLike]) -&gt; str:\n        \"\"\"\n        Creates or updates a mirror backup from the source directory.\n\n        A mirror backup differs from a full backup in several ways:\n\n        - Only one mirror backup can exist at a time (in the mirror_backup directory)\n        - Files are stored individually rather than in a tar archive\n        - Only files that have changed (based on hash comparison) are copied\n        - Files in the backup that no longer exist in the source are removed\n\n        Args:\n            source_path: Path to the directory that should be backed up\n\n        Returns:\n            str: Always returns \"mirror_backup\" as the backup identifier\n\n        !!! note\n            The backup's source directory name is stored in the manifest to help with\n            restoration. Empty directories in the backup that result from file deletions\n            are automatically removed.\n\n        Example:\n            ```python\n            manager = LocalBackupManager(backup_directory=\"/path/to/backups/\")\n            backup_name = manager.mirror_backup(source_path=\"/path/to/source_dir/\")\n            print(backup_name)\n            # Output: \"mirror_backup\"\n            ```\n        \"\"\"\n        params = MirrorBackupParams(source_path=source_path)\n        source_path = os.path.abspath(params.source_path)\n        source_dir = os.path.basename(source_path)\n\n        os.makedirs(self.mir_prefix, exist_ok=True)\n        current_mir_manifest = self._load_manifest()\n\n        local_stored_files = {}\n        for root, _, files in os.walk(self.mir_prefix):\n            for file in files:\n                if file == \"manifest.json\":\n                    continue\n                file_path = os.path.join(root, file)\n                rel_path = os.path.relpath(file_path, self.mir_prefix)\n                local_stored_files[rel_path] = file_path\n\n        local_files = {}\n        for root, _, files in os.walk(source_path):\n            for file in files:\n                file_path = os.path.join(root, file)\n                rel_path = os.path.relpath(file_path, source_path)\n                file_hash = self._get_file_hash(file_path) or \"\"\n                local_files[rel_path] = {\n                    \"path\": file_path,\n                    \"hash\": file_hash,\n                    \"mtime\": os.path.getmtime(file_path),\n                }\n\n        to_upload = []\n        to_delete = []\n\n        for rel_path, file_info in local_files.items():\n            if (\n                rel_path not in current_mir_manifest.files\n                or file_info[\"hash\"]\n                != current_mir_manifest.files.get(\n                    rel_path, FileInfo(hash=\"\", mtime=0)\n                ).hash\n            ):\n                to_upload.append((file_info[\"path\"], rel_path))\n\n        for rel_path in local_stored_files:\n            if rel_path not in local_files:\n                to_delete.append(local_stored_files[rel_path])\n\n        new_manifest = ManifestModel(\n            files={},\n            created_at=datetime.now().isoformat(),\n            type=\"mirror\",\n            source_directory=source_dir,\n        )\n\n        for rel_path, file_info in local_files.items():\n            new_manifest.files[rel_path] = FileInfo(\n                hash=file_info[\"hash\"], mtime=file_info[\"mtime\"]\n            )\n\n        for file_path, rel_path in to_upload:\n            target_path = os.path.join(self.mir_prefix, rel_path)\n            os.makedirs(os.path.dirname(target_path), exist_ok=True)\n            try:\n                shutil.copy2(file_path, target_path)\n            except Exception as e:\n                print(f\"Error copying {file_path}: {e}\")\n\n        for file_path in to_delete:\n            try:\n                os.remove(file_path)\n                dir_path = os.path.dirname(file_path)\n                while dir_path != self.mir_prefix:\n                    if not os.listdir(dir_path):\n                        os.rmdir(dir_path)\n                        dir_path = os.path.dirname(dir_path)\n                    else:\n                        break\n            except Exception as e:\n                print(f\"Error deleting {file_path}: {e}\")\n\n        self._save_manifest(new_manifest)\n        return \"mirror_backup\"\n\n    def restore(\n        self,\n        backup_name: str,\n        destination_path: Union[str, os.PathLike],\n        specific_files: Optional[List[str]] = None,\n    ) -&gt; bool:\n        \"\"\"\n        Restores files from a backup to a specified destination path.\n\n        This method supports restoring from both full and mirror backups:\n\n        - For mirror backups: Files are copied individually while preserving the directory structure\n        - For full backups: The tar.gz archive is extracted to the destination\n\n        Args:\n            backup_name: The name of the backup to restore from. Use \"mirror_backup\" for mirror backups or the directory name for full backups.\n            destination_path: The target directory where files will be restored to.\n            specific_files: Optional list of specific file paths to restore. If None, all files will be restored. Paths should be relative to the original backup source.\n\n        Returns:\n            bool: True if restoration succeeded, False if it failed.\n\n        !!! note\n            - For mirror backups, if the original source directory is stored in the manifest, a subdirectory with that name will be created at the destination.\n            - For full backups, the entire archive is extracted even when specific_files is used.\n            - Any errors during restoration are logged to stdout and will cause the method to return False.\n\n        Example:\n            ```python\n            manager = LocalBackupManager(backup_directory=\"/path/to/backups/\")\n            success = manager.restore(backup_name=\"my_backup\", destination_path=\"/path/to/restore_dir/\")\n            print(success)\n            # Output: True\n            ```\n        \"\"\"      \n        params = RestoreParams(\n            backup_name=backup_name,\n            destination_path=destination_path,\n            specific_files=set(specific_files) if specific_files else None,\n        )\n\n        os.makedirs(params.destination_path, exist_ok=True)\n\n        if params.backup_name == \"mirror_backup\":\n            manifest = self._load_manifest()\n            source_dir = manifest.source_directory or \"\"\n\n            if source_dir:\n                target_path = os.path.join(params.destination_path, source_dir)\n                os.makedirs(target_path, exist_ok=True)\n            else:\n                target_path = params.destination_path\n\n            try:\n                for root, _, files in os.walk(self.mir_prefix):\n                    for file in files:\n                        if file == \"manifest.json\":\n                            continue\n                        file_path = os.path.join(root, file)\n                        rel_path = os.path.relpath(file_path, self.mir_prefix)\n\n                        if (\n                            params.specific_files\n                            and rel_path not in params.specific_files\n                        ):\n                            continue\n\n                        if source_dir:\n                            local_path = os.path.join(target_path, rel_path)\n                        else:\n                            local_path = os.path.join(params.destination_path, rel_path)\n\n                        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n                        shutil.copy2(file_path, local_path)\n                return True\n            except Exception as e:\n                print(f\"Error during mirror restore: {e}\")\n                return False\n\n        backup_dir = os.path.join(self.full_prefix, params.backup_name)\n        archive_path = os.path.join(backup_dir, \"full_backup.tar.gz\")\n\n        if not os.path.exists(archive_path):\n            print(f\"Backup archive not found: {archive_path}\")\n            return False\n\n        try:\n            with tarfile.open(archive_path, \"r:gz\") as tar:\n                if params.specific_files:\n                    for member in tar.getmembers():\n                        if member.name in params.specific_files:\n                            tar.extractall(path=params.destination_path, filter=\"data\")\n                else:\n                    tar.extractall(path=params.destination_path, filter=\"data\")\n            return True\n        except Exception as e:\n            print(f\"Error during full backup restore: {e}\")\n            return False\n\n    def list_backups(self) -&gt; List[BackupInfo]:\n        \"\"\"\n        Lists all available backups in the backup directory.\n\n        This method scans both full backups and mirror backups:\n\n        - Full backups: Individual directories in the full_prefix location, each with a manifest.json\n        - Mirror backup: A single backup in the mirror_backup location with its own manifest.json\n\n        Returns:\n            List[BackupInfo]: A list of BackupInfo objects containing details about each backup:\n                - name: The name of the backup (directory name for full backups, \"mirror_backup\" for mirror)\n                - type: Either \"full\" or \"mirror\"\n                - created_at: ISO timestamp when the backup was created\n                - source_directory: Original source directory name (only for mirror backups)\n\n        !!! note\n            If errors occur while reading manifests, they are logged to stdout but don't interrupt \n            the listing process.\n\n        Example:\n            ```python\n            manager = LocalBackupManager(backup_directory=\"/path/to/backups/\")\n            backups = manager.list_backups()\n            for backup in backups:\n                print(f\"{backup.name} ({backup.type}) - Created: {backup.created_at}\")\n            ```\n        \"\"\"\n        backups = []\n        try:\n            for item in os.listdir(self.full_prefix):\n                item_path = os.path.join(self.full_prefix, item)\n                if os.path.isdir(item_path):\n                    manifest_path = os.path.join(item_path, \"manifest.json\")\n                    if os.path.exists(manifest_path):\n                        try:\n                            with open(manifest_path, \"r\") as f:\n                                manifest = json.load(f)\n                                backups.append(\n                                    BackupInfo(\n                                        name=item,\n                                        type=\"full\",\n                                        created_at=manifest.get(\n                                            \"created_at\", \"unknown\"\n                                        ),\n                                    )\n                                )\n                        except Exception as e:\n                            print(f\"Error reading manifest for {item}: {e}\")\n        except Exception as e:\n            print(f\"Error listing full backups: {e}\")\n\n        mir_manifest_path = self._get_mir_manifest_path()\n        if os.path.exists(mir_manifest_path):\n            try:\n                with open(mir_manifest_path, \"r\") as f:\n                    manifest = json.load(f)\n                    if manifest.get(\"files\"):\n                        backups.append(\n                            BackupInfo(\n                                name=\"mirror_backup\",\n                                type=\"mirror\",\n                                created_at=manifest.get(\"created_at\", \"unknown\"),\n                                source_directory=manifest.get(\n                                    \"source_directory\", \"unknown\"\n                                ),\n                            )\n                        )\n            except Exception as e:\n                print(f\"Error reading mirror backup manifest: {e}\")\n\n        return backups\n</code></pre>"},{"location":"api/api-reference/#pdldb.LocalBackupManager-functions","title":"Functions","text":""},{"location":"api/api-reference/#pdldb.LocalBackupManager.__init__","title":"<code>__init__(backup_directory, prefix='pdldb_backups/')</code>","text":"<p>Initialize a LocalBackupManager with the specified backup directory and prefix.</p> PARAMETER DESCRIPTION <code>backup_directory</code> <p>The base directory where all backups will be stored</p> <p> TYPE: <code>Union[str, PathLike]</code> </p> <code>prefix</code> <p>Optional prefix for backup subdirectories (default: \"pdldb_backups/\") Used to organize backups within the backup_directory</p> <p> TYPE: <code>str</code> DEFAULT: <code>'pdldb_backups/'</code> </p> <p>Note</p> <p>This method creates the necessary subdirectories for both full and mirror  backups if they don't already exist.</p> Source code in <code>src/pdldb/local_backup_manager.py</code> <pre><code>def __init__(\n    self, backup_directory: Union[str, os.PathLike], prefix: str = \"pdldb_backups/\"\n):\n    \"\"\"\n    Initialize a LocalBackupManager with the specified backup directory and prefix.\n\n    Args:\n        backup_directory: The base directory where all backups will be stored\n        prefix: Optional prefix for backup subdirectories (default: \"pdldb_backups/\")\n            Used to organize backups within the backup_directory\n\n    !!! note\n        This method creates the necessary subdirectories for both full and mirror \n        backups if they don't already exist.\n    \"\"\"\n    config = self.Config(\n        backup_directory=os.path.abspath(backup_directory), prefix=prefix\n    )\n    self.backup_directory = config.backup_directory\n    self.prefix = config.prefix\n    self.full_prefix = os.path.join(self.backup_directory, f\"{prefix}full_backups/\")\n    self.mir_prefix = os.path.join(self.backup_directory, f\"{prefix}mirror_backup/\")\n    os.makedirs(self.full_prefix, exist_ok=True)\n    os.makedirs(self.mir_prefix, exist_ok=True)\n</code></pre>"},{"location":"api/api-reference/#pdldb.LocalBackupManager.full_backup","title":"<code>full_backup(source_path, backup_name=None)</code>","text":"<p>Creates a complete backup of a source directory as a compressed archive.</p> <p>A full backup creates a new backup directory containing:</p> <ul> <li>A compressed tar.gz archive of the entire source directory</li> <li>A manifest.json file with metadata and file hashes for all files</li> </ul> <p>Unlike mirror backups, each full backup is stored as a separate archive, allowing for multiple backup versions to be maintained.</p> PARAMETER DESCRIPTION <code>source_path</code> <p>Path to the directory that should be backed up</p> <p> TYPE: <code>Union[str, PathLike]</code> </p> <code>backup_name</code> <p>Optional custom name for the backup. If not provided,         a name will be generated using the source directory name         and current timestamp (e.g., \"mydir_20250325_123045\")</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The name of the created backup (either the provided backup_name or the auto-generated name)</p> <p> TYPE: <code>str</code> </p> <p>Note</p> <ul> <li>If a backup with the specified name already exists, its contents will be overwritten.</li> <li>The manifest includes SHA-256 hashes and modification times for all files, which can be used for verification or restoration purposes.</li> </ul> Example <pre><code>manager = LocalBackupManager(backup_directory=\"/path/to/backups/\")\nbackup_name = manager.full_backup(source_path=\"/path/to/source_dir/\")\nprint(backup_name)\n# Output: \"source_dir_20250325_123045\"\n</code></pre> Source code in <code>src/pdldb/local_backup_manager.py</code> <pre><code>def full_backup(\n    self, source_path: Union[str, os.PathLike], backup_name: Optional[str] = None\n) -&gt; str:\n    \"\"\"\n    Creates a complete backup of a source directory as a compressed archive.\n\n    A full backup creates a new backup directory containing:\n\n    - A compressed tar.gz archive of the entire source directory\n    - A manifest.json file with metadata and file hashes for all files\n\n    Unlike mirror backups, each full backup is stored as a separate archive, allowing for multiple backup versions to be maintained.\n\n    Args:\n        source_path: Path to the directory that should be backed up\n        backup_name: Optional custom name for the backup. If not provided,\n                    a name will be generated using the source directory name\n                    and current timestamp (e.g., \"mydir_20250325_123045\")\n\n    Returns:\n        str: The name of the created backup (either the provided backup_name\n            or the auto-generated name)\n\n    !!! note\n        - If a backup with the specified name already exists, its contents will be overwritten.\n        - The manifest includes SHA-256 hashes and modification times for all files,\n        which can be used for verification or restoration purposes.\n\n    Example:\n        ```python\n        manager = LocalBackupManager(backup_directory=\"/path/to/backups/\")\n        backup_name = manager.full_backup(source_path=\"/path/to/source_dir/\")\n        print(backup_name)\n        # Output: \"source_dir_20250325_123045\"\n        ```\n    \"\"\"\n    params = FullBackupParams(source_path=source_path, backup_name=backup_name)\n\n    source_path = os.path.abspath(params.source_path)\n    source_dir = os.path.basename(source_path)\n\n    if not params.backup_name:\n        date_suffix = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        backup_name = f\"{source_dir}_{date_suffix}\"\n    else:\n        backup_name = params.backup_name\n\n    backup_dir = os.path.join(self.full_prefix, backup_name)\n    os.makedirs(backup_dir, exist_ok=True)\n    archive_path = os.path.join(backup_dir, \"full_backup.tar.gz\")\n\n    with tarfile.open(archive_path, \"w:gz\") as tar:\n        tar.add(source_path, arcname=os.path.basename(source_path))\n\n    manifest = ManifestModel(\n        files={}, created_at=datetime.now().isoformat(), type=\"full\"\n    )\n\n    for root, _, files in os.walk(source_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            rel_path = os.path.relpath(file_path, source_path)\n            manifest.files[rel_path] = FileInfo(\n                hash=self._get_file_hash(file_path) or \"\",\n                mtime=os.path.getmtime(file_path),\n            )\n\n    self._save_manifest(manifest, backup_name)\n    return backup_name\n</code></pre>"},{"location":"api/api-reference/#pdldb.LocalBackupManager.list_backups","title":"<code>list_backups()</code>","text":"<p>Lists all available backups in the backup directory.</p> <p>This method scans both full backups and mirror backups:</p> <ul> <li>Full backups: Individual directories in the full_prefix location, each with a manifest.json</li> <li>Mirror backup: A single backup in the mirror_backup location with its own manifest.json</li> </ul> RETURNS DESCRIPTION <code>List[BackupInfo]</code> <p>List[BackupInfo]: A list of BackupInfo objects containing details about each backup: - name: The name of the backup (directory name for full backups, \"mirror_backup\" for mirror) - type: Either \"full\" or \"mirror\" - created_at: ISO timestamp when the backup was created - source_directory: Original source directory name (only for mirror backups)</p> <p>Note</p> <p>If errors occur while reading manifests, they are logged to stdout but don't interrupt  the listing process.</p> Example <pre><code>manager = LocalBackupManager(backup_directory=\"/path/to/backups/\")\nbackups = manager.list_backups()\nfor backup in backups:\n    print(f\"{backup.name} ({backup.type}) - Created: {backup.created_at}\")\n</code></pre> Source code in <code>src/pdldb/local_backup_manager.py</code> <pre><code>def list_backups(self) -&gt; List[BackupInfo]:\n    \"\"\"\n    Lists all available backups in the backup directory.\n\n    This method scans both full backups and mirror backups:\n\n    - Full backups: Individual directories in the full_prefix location, each with a manifest.json\n    - Mirror backup: A single backup in the mirror_backup location with its own manifest.json\n\n    Returns:\n        List[BackupInfo]: A list of BackupInfo objects containing details about each backup:\n            - name: The name of the backup (directory name for full backups, \"mirror_backup\" for mirror)\n            - type: Either \"full\" or \"mirror\"\n            - created_at: ISO timestamp when the backup was created\n            - source_directory: Original source directory name (only for mirror backups)\n\n    !!! note\n        If errors occur while reading manifests, they are logged to stdout but don't interrupt \n        the listing process.\n\n    Example:\n        ```python\n        manager = LocalBackupManager(backup_directory=\"/path/to/backups/\")\n        backups = manager.list_backups()\n        for backup in backups:\n            print(f\"{backup.name} ({backup.type}) - Created: {backup.created_at}\")\n        ```\n    \"\"\"\n    backups = []\n    try:\n        for item in os.listdir(self.full_prefix):\n            item_path = os.path.join(self.full_prefix, item)\n            if os.path.isdir(item_path):\n                manifest_path = os.path.join(item_path, \"manifest.json\")\n                if os.path.exists(manifest_path):\n                    try:\n                        with open(manifest_path, \"r\") as f:\n                            manifest = json.load(f)\n                            backups.append(\n                                BackupInfo(\n                                    name=item,\n                                    type=\"full\",\n                                    created_at=manifest.get(\n                                        \"created_at\", \"unknown\"\n                                    ),\n                                )\n                            )\n                    except Exception as e:\n                        print(f\"Error reading manifest for {item}: {e}\")\n    except Exception as e:\n        print(f\"Error listing full backups: {e}\")\n\n    mir_manifest_path = self._get_mir_manifest_path()\n    if os.path.exists(mir_manifest_path):\n        try:\n            with open(mir_manifest_path, \"r\") as f:\n                manifest = json.load(f)\n                if manifest.get(\"files\"):\n                    backups.append(\n                        BackupInfo(\n                            name=\"mirror_backup\",\n                            type=\"mirror\",\n                            created_at=manifest.get(\"created_at\", \"unknown\"),\n                            source_directory=manifest.get(\n                                \"source_directory\", \"unknown\"\n                            ),\n                        )\n                    )\n        except Exception as e:\n            print(f\"Error reading mirror backup manifest: {e}\")\n\n    return backups\n</code></pre>"},{"location":"api/api-reference/#pdldb.LocalBackupManager.mirror_backup","title":"<code>mirror_backup(source_path)</code>","text":"<p>Creates or updates a mirror backup from the source directory.</p> <p>A mirror backup differs from a full backup in several ways:</p> <ul> <li>Only one mirror backup can exist at a time (in the mirror_backup directory)</li> <li>Files are stored individually rather than in a tar archive</li> <li>Only files that have changed (based on hash comparison) are copied</li> <li>Files in the backup that no longer exist in the source are removed</li> </ul> PARAMETER DESCRIPTION <code>source_path</code> <p>Path to the directory that should be backed up</p> <p> TYPE: <code>Union[str, PathLike]</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Always returns \"mirror_backup\" as the backup identifier</p> <p> TYPE: <code>str</code> </p> <p>Note</p> <p>The backup's source directory name is stored in the manifest to help with restoration. Empty directories in the backup that result from file deletions are automatically removed.</p> Example <pre><code>manager = LocalBackupManager(backup_directory=\"/path/to/backups/\")\nbackup_name = manager.mirror_backup(source_path=\"/path/to/source_dir/\")\nprint(backup_name)\n# Output: \"mirror_backup\"\n</code></pre> Source code in <code>src/pdldb/local_backup_manager.py</code> <pre><code>def mirror_backup(self, source_path: Union[str, os.PathLike]) -&gt; str:\n    \"\"\"\n    Creates or updates a mirror backup from the source directory.\n\n    A mirror backup differs from a full backup in several ways:\n\n    - Only one mirror backup can exist at a time (in the mirror_backup directory)\n    - Files are stored individually rather than in a tar archive\n    - Only files that have changed (based on hash comparison) are copied\n    - Files in the backup that no longer exist in the source are removed\n\n    Args:\n        source_path: Path to the directory that should be backed up\n\n    Returns:\n        str: Always returns \"mirror_backup\" as the backup identifier\n\n    !!! note\n        The backup's source directory name is stored in the manifest to help with\n        restoration. Empty directories in the backup that result from file deletions\n        are automatically removed.\n\n    Example:\n        ```python\n        manager = LocalBackupManager(backup_directory=\"/path/to/backups/\")\n        backup_name = manager.mirror_backup(source_path=\"/path/to/source_dir/\")\n        print(backup_name)\n        # Output: \"mirror_backup\"\n        ```\n    \"\"\"\n    params = MirrorBackupParams(source_path=source_path)\n    source_path = os.path.abspath(params.source_path)\n    source_dir = os.path.basename(source_path)\n\n    os.makedirs(self.mir_prefix, exist_ok=True)\n    current_mir_manifest = self._load_manifest()\n\n    local_stored_files = {}\n    for root, _, files in os.walk(self.mir_prefix):\n        for file in files:\n            if file == \"manifest.json\":\n                continue\n            file_path = os.path.join(root, file)\n            rel_path = os.path.relpath(file_path, self.mir_prefix)\n            local_stored_files[rel_path] = file_path\n\n    local_files = {}\n    for root, _, files in os.walk(source_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            rel_path = os.path.relpath(file_path, source_path)\n            file_hash = self._get_file_hash(file_path) or \"\"\n            local_files[rel_path] = {\n                \"path\": file_path,\n                \"hash\": file_hash,\n                \"mtime\": os.path.getmtime(file_path),\n            }\n\n    to_upload = []\n    to_delete = []\n\n    for rel_path, file_info in local_files.items():\n        if (\n            rel_path not in current_mir_manifest.files\n            or file_info[\"hash\"]\n            != current_mir_manifest.files.get(\n                rel_path, FileInfo(hash=\"\", mtime=0)\n            ).hash\n        ):\n            to_upload.append((file_info[\"path\"], rel_path))\n\n    for rel_path in local_stored_files:\n        if rel_path not in local_files:\n            to_delete.append(local_stored_files[rel_path])\n\n    new_manifest = ManifestModel(\n        files={},\n        created_at=datetime.now().isoformat(),\n        type=\"mirror\",\n        source_directory=source_dir,\n    )\n\n    for rel_path, file_info in local_files.items():\n        new_manifest.files[rel_path] = FileInfo(\n            hash=file_info[\"hash\"], mtime=file_info[\"mtime\"]\n        )\n\n    for file_path, rel_path in to_upload:\n        target_path = os.path.join(self.mir_prefix, rel_path)\n        os.makedirs(os.path.dirname(target_path), exist_ok=True)\n        try:\n            shutil.copy2(file_path, target_path)\n        except Exception as e:\n            print(f\"Error copying {file_path}: {e}\")\n\n    for file_path in to_delete:\n        try:\n            os.remove(file_path)\n            dir_path = os.path.dirname(file_path)\n            while dir_path != self.mir_prefix:\n                if not os.listdir(dir_path):\n                    os.rmdir(dir_path)\n                    dir_path = os.path.dirname(dir_path)\n                else:\n                    break\n        except Exception as e:\n            print(f\"Error deleting {file_path}: {e}\")\n\n    self._save_manifest(new_manifest)\n    return \"mirror_backup\"\n</code></pre>"},{"location":"api/api-reference/#pdldb.LocalBackupManager.restore","title":"<code>restore(backup_name, destination_path, specific_files=None)</code>","text":"<p>Restores files from a backup to a specified destination path.</p> <p>This method supports restoring from both full and mirror backups:</p> <ul> <li>For mirror backups: Files are copied individually while preserving the directory structure</li> <li>For full backups: The tar.gz archive is extracted to the destination</li> </ul> PARAMETER DESCRIPTION <code>backup_name</code> <p>The name of the backup to restore from. Use \"mirror_backup\" for mirror backups or the directory name for full backups.</p> <p> TYPE: <code>str</code> </p> <code>destination_path</code> <p>The target directory where files will be restored to.</p> <p> TYPE: <code>Union[str, PathLike]</code> </p> <code>specific_files</code> <p>Optional list of specific file paths to restore. If None, all files will be restored. Paths should be relative to the original backup source.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if restoration succeeded, False if it failed.</p> <p> TYPE: <code>bool</code> </p> <p>Note</p> <ul> <li>For mirror backups, if the original source directory is stored in the manifest, a subdirectory with that name will be created at the destination.</li> <li>For full backups, the entire archive is extracted even when specific_files is used.</li> <li>Any errors during restoration are logged to stdout and will cause the method to return False.</li> </ul> Example <pre><code>manager = LocalBackupManager(backup_directory=\"/path/to/backups/\")\nsuccess = manager.restore(backup_name=\"my_backup\", destination_path=\"/path/to/restore_dir/\")\nprint(success)\n# Output: True\n</code></pre> Source code in <code>src/pdldb/local_backup_manager.py</code> <pre><code>def restore(\n    self,\n    backup_name: str,\n    destination_path: Union[str, os.PathLike],\n    specific_files: Optional[List[str]] = None,\n) -&gt; bool:\n    \"\"\"\n    Restores files from a backup to a specified destination path.\n\n    This method supports restoring from both full and mirror backups:\n\n    - For mirror backups: Files are copied individually while preserving the directory structure\n    - For full backups: The tar.gz archive is extracted to the destination\n\n    Args:\n        backup_name: The name of the backup to restore from. Use \"mirror_backup\" for mirror backups or the directory name for full backups.\n        destination_path: The target directory where files will be restored to.\n        specific_files: Optional list of specific file paths to restore. If None, all files will be restored. Paths should be relative to the original backup source.\n\n    Returns:\n        bool: True if restoration succeeded, False if it failed.\n\n    !!! note\n        - For mirror backups, if the original source directory is stored in the manifest, a subdirectory with that name will be created at the destination.\n        - For full backups, the entire archive is extracted even when specific_files is used.\n        - Any errors during restoration are logged to stdout and will cause the method to return False.\n\n    Example:\n        ```python\n        manager = LocalBackupManager(backup_directory=\"/path/to/backups/\")\n        success = manager.restore(backup_name=\"my_backup\", destination_path=\"/path/to/restore_dir/\")\n        print(success)\n        # Output: True\n        ```\n    \"\"\"      \n    params = RestoreParams(\n        backup_name=backup_name,\n        destination_path=destination_path,\n        specific_files=set(specific_files) if specific_files else None,\n    )\n\n    os.makedirs(params.destination_path, exist_ok=True)\n\n    if params.backup_name == \"mirror_backup\":\n        manifest = self._load_manifest()\n        source_dir = manifest.source_directory or \"\"\n\n        if source_dir:\n            target_path = os.path.join(params.destination_path, source_dir)\n            os.makedirs(target_path, exist_ok=True)\n        else:\n            target_path = params.destination_path\n\n        try:\n            for root, _, files in os.walk(self.mir_prefix):\n                for file in files:\n                    if file == \"manifest.json\":\n                        continue\n                    file_path = os.path.join(root, file)\n                    rel_path = os.path.relpath(file_path, self.mir_prefix)\n\n                    if (\n                        params.specific_files\n                        and rel_path not in params.specific_files\n                    ):\n                        continue\n\n                    if source_dir:\n                        local_path = os.path.join(target_path, rel_path)\n                    else:\n                        local_path = os.path.join(params.destination_path, rel_path)\n\n                    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n                    shutil.copy2(file_path, local_path)\n            return True\n        except Exception as e:\n            print(f\"Error during mirror restore: {e}\")\n            return False\n\n    backup_dir = os.path.join(self.full_prefix, params.backup_name)\n    archive_path = os.path.join(backup_dir, \"full_backup.tar.gz\")\n\n    if not os.path.exists(archive_path):\n        print(f\"Backup archive not found: {archive_path}\")\n        return False\n\n    try:\n        with tarfile.open(archive_path, \"r:gz\") as tar:\n            if params.specific_files:\n                for member in tar.getmembers():\n                    if member.name in params.specific_files:\n                        tar.extractall(path=params.destination_path, filter=\"data\")\n            else:\n                tar.extractall(path=params.destination_path, filter=\"data\")\n        return True\n    except Exception as e:\n        print(f\"Error during full backup restore: {e}\")\n        return False\n</code></pre>"},{"location":"api/api-reference/#pdldb.S3BackupManager","title":"<code>pdldb.S3BackupManager</code>","text":"<p>Manages file backups to Amazon S3 or compatible storage with support for  both full and mirror backup strategies.</p> <p>This class provides cloud-based functionality to:</p> <ul> <li>Create full backups (complete tar.gz archives with manifests)</li> <li>Create/update mirror backups (individual files with change detection)</li> <li>Restore files from either backup type</li> <li>List available backups with their metadata</li> </ul> <p>All backups are stored in a structured S3 key hierarchy:</p> <ul> <li>{prefix}full_backups/ - Contains all full backups</li> <li>{prefix}mirror_backup/ - Contains the single mirror backup</li> </ul> <p>Each backup includes a manifest.json file with metadata about the backup, including  file hashes, modification times, creation timestamp, and backup type.</p> Source code in <code>src/pdldb/s3_backup_manager.py</code> <pre><code>class S3BackupManager:\n    \"\"\"\n    Manages file backups to Amazon S3 or compatible storage with support for \n    both full and mirror backup strategies.\n\n    This class provides cloud-based functionality to:\n\n    - Create full backups (complete tar.gz archives with manifests)\n    - Create/update mirror backups (individual files with change detection)\n    - Restore files from either backup type\n    - List available backups with their metadata\n\n    All backups are stored in a structured S3 key hierarchy:\n\n    - {prefix}full_backups/ - Contains all full backups\n    - {prefix}mirror_backup/ - Contains the single mirror backup\n\n    Each backup includes a manifest.json file with metadata about the backup, including \n    file hashes, modification times, creation timestamp, and backup type.\n    \"\"\"\n    def __init__(\n        self,\n        bucket_name: Optional[str] = None,\n        aws_region: Optional[str] = None,\n        prefix: str = \"pdldb_backups/\",\n        endpoint_url: Optional[str] = None,\n        aws_access_key_id: Optional[str] = None,\n        aws_secret_access_key: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize an S3BackupManager with the specified S3 bucket and configuration.\n\n        Args:\n            bucket_name: S3 bucket where backups will be stored. If None, will use S3_BUCKET_NAME env var.\n            aws_region: AWS region for the S3 bucket. If None, will use AWS_REGION or AWS_DEFAULT_REGION env vars.\n            prefix: Optional prefix for backup keys in the bucket (default: \"pdldb_backups/\")\n            endpoint_url: Optional endpoint URL for S3-compatible storage (e.g., MinIO)\n            aws_access_key_id: AWS access key ID. If None, will use AWS_ACCESS_KEY_ID env var.\n            aws_secret_access_key: AWS secret access key. If None, will use AWS_SECRET_ACCESS_KEY env var.\n\n        !!! note\n            This method sets up the S3 client and prepares the necessary key prefixes for\n            both full and mirror backups.\n\n        Example:\n            ```python\n            backup_manager = S3BackupManager(bucket_name=\"my-bucket\")\n            ```\n        \"\"\"\n        config = S3BackupConfig(\n            bucket_name=bucket_name,\n            aws_region=aws_region,\n            prefix=prefix,\n            endpoint_url=endpoint_url,\n            aws_access_key_id=aws_access_key_id,\n            aws_secret_access_key=aws_secret_access_key,\n        )\n\n        self.s3_client = boto3.client(\n            \"s3\",\n            region_name=config.aws_region\n            or os.environ.get(\"AWS_REGION\")\n            or os.environ.get(\"AWS_DEFAULT_REGION\"),\n            endpoint_url=config.endpoint_url,\n            aws_access_key_id=config.aws_access_key_id\n            or os.environ.get(\"AWS_ACCESS_KEY_ID\"),\n            aws_secret_access_key=config.aws_secret_access_key\n            or os.environ.get(\"AWS_SECRET_ACCESS_KEY\"),\n        )\n        self.bucket = config.bucket_name or os.environ.get(\"S3_BUCKET_NAME\")\n        self.prefix = config.prefix\n        self.full_prefix = f\"{config.prefix}full_backups/\"\n        self.mir_prefix = f\"{config.prefix}mirror_backup/\"\n\n        if not self.bucket:\n            raise ValueError(\n                \"S3 bucket name is required. Provide it as a parameter or set S3_BUCKET_NAME environment variable.\"\n            )\n\n    def _get_file_hash(self, filepath):\n        if not os.path.isfile(filepath):\n            return None\n\n        hasher = hashlib.sha256()\n        with open(filepath, \"rb\") as f:\n            for chunk in iter(lambda: f.read(4096), b\"\"):\n                hasher.update(chunk)\n        return hasher.hexdigest()\n\n    def _get_manifest_key(self, backup_name):\n        return f\"{self.full_prefix}{backup_name}/manifest.json\"\n\n    def _get_mir_manifest_key(self):\n        return f\"{self.mir_prefix}manifest.json\"\n\n    def _load_manifest(self, backup_name=None):\n        if backup_name:\n            try:\n                response = self.s3_client.get_object(\n                    Bucket=self.bucket, Key=self._get_manifest_key(backup_name)\n                )\n                return json.loads(response[\"Body\"].read().decode(\"utf-8\"))\n            except ClientError:\n                return {\"files\": {}, \"created_at\": datetime.now().isoformat()}\n        else:\n            try:\n                response = self.s3_client.get_object(\n                    Bucket=self.bucket, Key=self._get_mir_manifest_key()\n                )\n                return json.loads(response[\"Body\"].read().decode(\"utf-8\"))\n            except ClientError:\n                return {\"files\": {}, \"created_at\": datetime.now().isoformat()}\n\n    def _save_manifest(self, manifest, backup_name=None):\n        manifest_data = json.dumps(manifest).encode(\"utf-8\")\n        if backup_name:\n            self.s3_client.put_object(\n                Bucket=self.bucket,\n                Key=self._get_manifest_key(backup_name),\n                Body=manifest_data,\n            )\n        else:\n            self.s3_client.put_object(\n                Bucket=self.bucket, Key=self._get_mir_manifest_key(), Body=manifest_data\n            )\n\n    def full_backup(self, source_path: str, backup_name: Optional[str] = None) -&gt; str:\n        \"\"\"\n        Creates a complete backup of a source directory as a compressed archive in S3.\n\n        A full backup creates:\n\n        - A compressed tar.gz archive of the entire source directory uploaded to S3\n        - A manifest.json file with metadata and file hashes for all files\n\n        Unlike mirror backups, each full backup is stored as a separate archive,\n        allowing for multiple backup versions to be maintained.\n\n        Args:\n            source_path: Path to the directory that should be backed up\n            backup_name: Optional custom name for the backup. If not provided,\n                        a name will be generated using the source directory name\n                        and current timestamp (e.g., \"mydir_20250325_123045\")\n\n        Returns:\n            str: The name of the created backup (either the provided backup_name\n                or the auto-generated name)\n\n        !!! note\n            - If a backup with the specified name already exists in S3, it will be overwritten.\n            - The manifest includes SHA-256 hashes and modification times for all files, which can be used for verification or restoration purposes.\n            - The archive is created in a temporary file before being uploaded to S3.\n\n        Example:\n            ```python\n            backup_manager = S3BackupManager(bucket_name=\"my-bucket\")\n            backup_name = backup_manager.full_backup(\"/path/to/source\")\n            print(f\"Full backup created: {backup_name}\")\n            ```\n        \"\"\"\n        params = FullBackupRequest(source_path=source_path, backup_name=backup_name)\n\n        source_path = os.path.abspath(params.source_path)\n        source_dir = os.path.basename(source_path)\n\n        if not os.path.exists(source_path):\n            raise ValueError(f\"Source path does not exist: {source_path}\")\n\n        if not params.backup_name:\n            date_suffix = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            backup_name = f\"{source_dir}_{date_suffix}\"\n        else:\n            backup_name = params.backup_name\n\n        with tempfile.NamedTemporaryFile(suffix=\".tar.gz\", delete=False) as tmp_file:\n            archive_path = tmp_file.name\n\n        try:\n            with tarfile.open(archive_path, \"w:gz\") as tar:\n                tar.add(source_path, arcname=os.path.basename(source_path))\n\n            backup_key = f\"{self.full_prefix}{backup_name}/full_backup.tar.gz\"\n            self.s3_client.upload_file(\n                archive_path,\n                self.bucket,\n                backup_key,\n                ExtraArgs={\"StorageClass\": \"STANDARD\"},\n            )\n\n            created_at = datetime.now().isoformat()\n\n            manifest = {\n                \"files\": {},\n                \"created_at\": created_at,\n                \"type\": \"full\",\n            }\n\n            file_count = 0\n            for root, _, files in os.walk(source_path):\n                for file in files:\n                    file_path = os.path.join(root, file)\n                    rel_path = os.path.relpath(file_path, source_path)\n                    manifest[\"files\"][rel_path] = {\n                        \"hash\": self._get_file_hash(file_path),\n                        \"mtime\": os.path.getmtime(file_path),\n                    }\n                    file_count += 1\n\n            self._save_manifest(manifest, backup_name)\n\n            return backup_name\n\n        finally:\n            if os.path.exists(archive_path):\n                os.unlink(archive_path)\n\n    def mirror_backup(self, source_path: str) -&gt; str:\n        \"\"\"\n        Creates or updates a mirror backup from the source directory to S3.\n\n        A mirror backup differs from a full backup in several ways:\n\n        - Only one mirror backup can exist at a time (in the mirror_backup prefix)\n        - Files are stored individually rather than in a tar archive\n        - Only files that have changed (based on hash comparison) are uploaded\n        - Files in S3 that no longer exist in the source are removed\n\n        The method performs an incremental sync by:\n\n        1. Scanning the source directory and calculating file hashes\n        2. Comparing with the current S3 mirror backup manifest\n        3. Determining which files need to be added, updated, or removed\n        4. Uploading only the changed files and updating the manifest\n\n        Args:\n            source_path: Path to the directory that should be backed up\n\n        Returns:\n            str: Always returns \"mirror_backup\" as the backup identifier\n\n        !!! note\n            - The backup's source directory name is stored in the manifest to help with restoration\n            - S3 objects are deleted in batches of 1000 (S3 API limitation)\n            - Errors during upload or deletion are logged but don't interrupt the overall process\n\n        Example:\n            ```python\n            backup_manager = S3BackupManager(bucket_name=\"my-bucket\")\n            success = backup_manager.mirror_backup(\"/path/to/source\")\n            if success:\n                print(\"Mirror backup completed!\")\n            ```\n        \"\"\"\n        params = MirrorBackupRequest(source_path=source_path)\n        source_path = os.path.abspath(params.source_path)\n\n        if not os.path.exists(source_path):\n            raise ValueError(f\"Source path does not exist: {source_path}\")\n\n        source_dir = os.path.basename(source_path)\n\n        try:\n            current_mir_manifest = self._load_manifest()\n        except ClientError:\n            current_mir_manifest = {\n                \"files\": {},\n                \"created_at\": datetime.now().isoformat(),\n            }\n\n        s3_files = {}\n        try:\n            paginator = self.s3_client.get_paginator(\"list_objects_v2\")\n            pages = paginator.paginate(Bucket=self.bucket, Prefix=self.mir_prefix)\n\n            for page in pages:\n                for obj in page.get(\"Contents\", []):\n                    key = obj[\"Key\"]\n                    if not key.endswith(\"manifest.json\"):\n                        rel_path = key[len(self.mir_prefix) :]\n                        s3_files[rel_path] = key\n        except ClientError:\n            pass\n\n        local_files = {}\n        for root, _, files in os.walk(source_path):\n            for file in files:\n                file_path = os.path.join(root, file)\n                rel_path = os.path.relpath(file_path, source_path)\n                local_files[rel_path] = {\n                    \"path\": file_path,\n                    \"hash\": self._get_file_hash(file_path),\n                    \"mtime\": os.path.getmtime(file_path),\n                }\n\n        to_upload = []\n        to_delete = []\n\n        for rel_path, file_info in local_files.items():\n            if rel_path not in current_mir_manifest.get(\"files\", {}) or file_info[\n                \"hash\"\n            ] != current_mir_manifest[\"files\"].get(rel_path, {}).get(\"hash\"):\n                to_upload.append((file_info[\"path\"], rel_path))\n\n        for rel_path in s3_files:\n            if rel_path not in local_files:\n                to_delete.append({\"Key\": s3_files[rel_path]})\n\n        new_manifest = {\n            \"files\": {},\n            \"created_at\": datetime.now().isoformat(),\n            \"type\": \"mirror\",\n            \"source_directory\": source_dir,\n        }\n\n        for rel_path, file_info in local_files.items():\n            new_manifest[\"files\"][rel_path] = {\n                \"hash\": file_info[\"hash\"],\n                \"mtime\": file_info[\"mtime\"],\n            }\n\n        for file_path, rel_path in to_upload:\n            target_key = f\"{self.mir_prefix}{rel_path}\"\n            try:\n                self.s3_client.upload_file(\n                    file_path,\n                    self.bucket,\n                    target_key,\n                    ExtraArgs={\"StorageClass\": \"STANDARD\"},\n                )\n            except Exception as e:\n                print(f\"Error uploading {file_path}: {e}\")\n\n        if to_delete:\n            try:\n                for i in range(0, len(to_delete), 1000):\n                    batch = to_delete[i : i + 1000]\n                    self.s3_client.delete_objects(\n                        Bucket=self.bucket, Delete={\"Objects\": batch, \"Quiet\": True}\n                    )\n            except Exception as e:\n                print(f\"Error deleting objects: {e}\")\n\n        self._save_manifest(new_manifest)\n\n        return \"mirror_backup\"\n\n    def restore(\n        self,\n        backup_name: str,\n        destination_path: str,\n        specific_files: Optional[List[str]] = None,\n    ) -&gt; bool:\n        \"\"\"\n        Restores files from an S3 backup to a specified local destination path.\n\n        This method supports restoring from both full and mirror backups:\n\n        - For mirror backups: Files are downloaded individually while preserving the directory structure\n        - For full backups: The tar.gz archive is downloaded and extracted to the destination\n\n        Args:\n            backup_name: The name of the backup to restore from. Use \"mirror_backup\" for mirror backups\n                        or the directory name for full backups.\n            destination_path: The target directory where files will be restored to.\n            specific_files: Optional list of specific file paths to restore. If None, all files \n                            will be restored. Paths should be relative to the original backup source.\n\n        Returns:\n            bool: True if restoration succeeded, False if it failed.\n\n        !!! note\n            - For mirror backups, if the original source directory is stored in the manifest, a subdirectory with that name will be created at the destination.\n            - For full backups, the entire archive is downloaded to a temporary directory before extracting the specified files or the entire contents.\n            - Any errors during restoration are logged to stdout and will cause the method to return False.\n\n        Example:\n            ```python\n            backup_manager = S3BackupManager(bucket_name=\"my-bucket\")\n            success = backup_manager.restore(\"my_backup\", \"/restore/path\")\n            if success:\n                print(\"Restoration successful!\")\n            ```\n        \"\"\"\n        params = RestoreRequest(\n            backup_name=backup_name,\n            destination_path=destination_path,\n            specific_files=specific_files,\n        )\n\n        destination_path = params.destination_path\n        backup_name = params.backup_name\n        specific_files = params.specific_files\n\n        os.makedirs(destination_path, exist_ok=True)\n\n        if backup_name == \"mirror_backup\":\n            manifest = self._load_manifest()\n            files_prefix = self.mir_prefix\n\n            source_dir = manifest.get(\"source_directory\", \"\")\n\n            if source_dir:\n                target_path = os.path.join(destination_path, source_dir)\n                os.makedirs(target_path, exist_ok=True)\n            else:\n                target_path = destination_path\n\n            try:\n                paginator = self.s3_client.get_paginator(\"list_objects_v2\")\n                pages = paginator.paginate(Bucket=self.bucket, Prefix=files_prefix)\n\n                for page in pages:\n                    for obj in page.get(\"Contents\", []):\n                        file_key = obj[\"Key\"]\n                        if file_key.endswith(\"manifest.json\"):\n                            continue\n\n                        rel_path = file_key[len(files_prefix) :]\n\n                        if specific_files and rel_path not in specific_files:\n                            continue\n\n                        if source_dir:\n                            local_path = os.path.join(target_path, rel_path)\n                        else:\n                            local_path = os.path.join(destination_path, rel_path)\n\n                        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n\n                        self.s3_client.download_file(self.bucket, file_key, local_path)\n\n                return True\n            except ClientError as e:\n                print(f\"Error during mirror restore: {e}\")\n                return False\n\n        manifest = self._load_manifest(backup_name)\n\n        with tempfile.TemporaryDirectory() as temp_dir:\n            archive_key = f\"{self.full_prefix}{backup_name}/full_backup.tar.gz\"\n            archive_path = os.path.join(temp_dir, f\"{backup_name}.tar.gz\")\n\n            try:\n                self.s3_client.download_file(self.bucket, archive_key, archive_path)\n\n                with tarfile.open(archive_path, \"r:gz\") as tar:\n                    if specific_files:\n                        for member in tar.getmembers():\n                            if member.name in specific_files:\n                                tar.extract(member, path=destination_path)\n                    else:\n                        tar.extractall(path=destination_path)\n\n                return True\n\n            except ClientError:\n                return False\n\n        return False\n\n    def list_backups(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Lists all available backups in the S3 bucket under the configured prefix.\n\n        This method scans both full backups and mirror backups:\n\n        - Full backups: S3 prefixes under full_backups/, each with a manifest.json\n        - Mirror backup: A single backup in the mirror_backup/ prefix with its own manifest.json\n\n        Returns:\n            List[Dict[str, Any]]: A list of dictionaries containing details about each backup:\n                - name: The name of the backup (prefix name for full backups, \"mirror_backup\" for mirror)\n                - type: Either \"full\" or \"mirror\"\n                - created_at: ISO timestamp when the backup was created\n                - source_directory: Original source directory name (only for mirror backups)\n\n        !!! note\n            If errors occur while fetching objects or parsing manifests, they are logged to stdout\n            but don't interrupt the listing process. The S3 list operation uses pagination to handle\n            potentially large numbers of backups.\n\n        Example:\n            ```python\n            backup_manager = S3BackupManager(bucket_name=\"my-bucket\")\n            backups = backup_manager.list_backups()\n            for backup in backups:\n                print(f\"{backup['name']} ({backup['type']}): {backup['created_at']}\")\n            ```\n        \"\"\"\n        backups = []\n\n        try:\n            response = self.s3_client.list_objects_v2(\n                Bucket=self.bucket, Prefix=self.full_prefix, Delimiter=\"/\"\n            )\n\n            for prefix in response.get(\"CommonPrefixes\", []):\n                backup_name = prefix[\"Prefix\"].split(\"/\")[-2]\n                try:\n                    manifest = self._load_manifest(backup_name)\n                    backups.append(\n                        {\n                            \"name\": backup_name,\n                            \"type\": \"full\",\n                            \"created_at\": manifest.get(\"created_at\", \"unknown\"),\n                        }\n                    )\n                except ClientError as e:\n                    print(f\"Error loading manifest for {backup_name}: {e}\")\n                except Exception as e:\n                    print(f\"Unexpected error processing backup {backup_name}: {e}\")\n\n        except ClientError as e:\n            print(f\"Error listing full backups: {e}\")\n\n        try:\n            manifest = self._load_manifest()\n            if manifest.get(\"files\"):\n                backups.append(\n                    {\n                        \"name\": \"mirror_backup\",\n                        \"type\": \"mirror\",\n                        \"created_at\": manifest.get(\"created_at\", \"unknown\"),\n                        \"source_directory\": manifest.get(\"source_directory\", \"unknown\"),\n                    }\n                )\n        except ClientError as e:\n            print(f\"Error loading mirror backup manifest: {e}\")\n        except Exception as e:\n            print(f\"Unexpected error processing mirror backup: {e}\")\n\n        return backups\n</code></pre>"},{"location":"api/api-reference/#pdldb.S3BackupManager-functions","title":"Functions","text":""},{"location":"api/api-reference/#pdldb.S3BackupManager.__init__","title":"<code>__init__(bucket_name=None, aws_region=None, prefix='pdldb_backups/', endpoint_url=None, aws_access_key_id=None, aws_secret_access_key=None)</code>","text":"<p>Initialize an S3BackupManager with the specified S3 bucket and configuration.</p> PARAMETER DESCRIPTION <code>bucket_name</code> <p>S3 bucket where backups will be stored. If None, will use S3_BUCKET_NAME env var.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>aws_region</code> <p>AWS region for the S3 bucket. If None, will use AWS_REGION or AWS_DEFAULT_REGION env vars.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>prefix</code> <p>Optional prefix for backup keys in the bucket (default: \"pdldb_backups/\")</p> <p> TYPE: <code>str</code> DEFAULT: <code>'pdldb_backups/'</code> </p> <code>endpoint_url</code> <p>Optional endpoint URL for S3-compatible storage (e.g., MinIO)</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>aws_access_key_id</code> <p>AWS access key ID. If None, will use AWS_ACCESS_KEY_ID env var.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>aws_secret_access_key</code> <p>AWS secret access key. If None, will use AWS_SECRET_ACCESS_KEY env var.</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <p>Note</p> <p>This method sets up the S3 client and prepares the necessary key prefixes for both full and mirror backups.</p> Example <pre><code>backup_manager = S3BackupManager(bucket_name=\"my-bucket\")\n</code></pre> Source code in <code>src/pdldb/s3_backup_manager.py</code> <pre><code>def __init__(\n    self,\n    bucket_name: Optional[str] = None,\n    aws_region: Optional[str] = None,\n    prefix: str = \"pdldb_backups/\",\n    endpoint_url: Optional[str] = None,\n    aws_access_key_id: Optional[str] = None,\n    aws_secret_access_key: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize an S3BackupManager with the specified S3 bucket and configuration.\n\n    Args:\n        bucket_name: S3 bucket where backups will be stored. If None, will use S3_BUCKET_NAME env var.\n        aws_region: AWS region for the S3 bucket. If None, will use AWS_REGION or AWS_DEFAULT_REGION env vars.\n        prefix: Optional prefix for backup keys in the bucket (default: \"pdldb_backups/\")\n        endpoint_url: Optional endpoint URL for S3-compatible storage (e.g., MinIO)\n        aws_access_key_id: AWS access key ID. If None, will use AWS_ACCESS_KEY_ID env var.\n        aws_secret_access_key: AWS secret access key. If None, will use AWS_SECRET_ACCESS_KEY env var.\n\n    !!! note\n        This method sets up the S3 client and prepares the necessary key prefixes for\n        both full and mirror backups.\n\n    Example:\n        ```python\n        backup_manager = S3BackupManager(bucket_name=\"my-bucket\")\n        ```\n    \"\"\"\n    config = S3BackupConfig(\n        bucket_name=bucket_name,\n        aws_region=aws_region,\n        prefix=prefix,\n        endpoint_url=endpoint_url,\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key,\n    )\n\n    self.s3_client = boto3.client(\n        \"s3\",\n        region_name=config.aws_region\n        or os.environ.get(\"AWS_REGION\")\n        or os.environ.get(\"AWS_DEFAULT_REGION\"),\n        endpoint_url=config.endpoint_url,\n        aws_access_key_id=config.aws_access_key_id\n        or os.environ.get(\"AWS_ACCESS_KEY_ID\"),\n        aws_secret_access_key=config.aws_secret_access_key\n        or os.environ.get(\"AWS_SECRET_ACCESS_KEY\"),\n    )\n    self.bucket = config.bucket_name or os.environ.get(\"S3_BUCKET_NAME\")\n    self.prefix = config.prefix\n    self.full_prefix = f\"{config.prefix}full_backups/\"\n    self.mir_prefix = f\"{config.prefix}mirror_backup/\"\n\n    if not self.bucket:\n        raise ValueError(\n            \"S3 bucket name is required. Provide it as a parameter or set S3_BUCKET_NAME environment variable.\"\n        )\n</code></pre>"},{"location":"api/api-reference/#pdldb.S3BackupManager.full_backup","title":"<code>full_backup(source_path, backup_name=None)</code>","text":"<p>Creates a complete backup of a source directory as a compressed archive in S3.</p> <p>A full backup creates:</p> <ul> <li>A compressed tar.gz archive of the entire source directory uploaded to S3</li> <li>A manifest.json file with metadata and file hashes for all files</li> </ul> <p>Unlike mirror backups, each full backup is stored as a separate archive, allowing for multiple backup versions to be maintained.</p> PARAMETER DESCRIPTION <code>source_path</code> <p>Path to the directory that should be backed up</p> <p> TYPE: <code>str</code> </p> <code>backup_name</code> <p>Optional custom name for the backup. If not provided,         a name will be generated using the source directory name         and current timestamp (e.g., \"mydir_20250325_123045\")</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>str</code> <p>The name of the created backup (either the provided backup_name or the auto-generated name)</p> <p> TYPE: <code>str</code> </p> <p>Note</p> <ul> <li>If a backup with the specified name already exists in S3, it will be overwritten.</li> <li>The manifest includes SHA-256 hashes and modification times for all files, which can be used for verification or restoration purposes.</li> <li>The archive is created in a temporary file before being uploaded to S3.</li> </ul> Example <pre><code>backup_manager = S3BackupManager(bucket_name=\"my-bucket\")\nbackup_name = backup_manager.full_backup(\"/path/to/source\")\nprint(f\"Full backup created: {backup_name}\")\n</code></pre> Source code in <code>src/pdldb/s3_backup_manager.py</code> <pre><code>def full_backup(self, source_path: str, backup_name: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Creates a complete backup of a source directory as a compressed archive in S3.\n\n    A full backup creates:\n\n    - A compressed tar.gz archive of the entire source directory uploaded to S3\n    - A manifest.json file with metadata and file hashes for all files\n\n    Unlike mirror backups, each full backup is stored as a separate archive,\n    allowing for multiple backup versions to be maintained.\n\n    Args:\n        source_path: Path to the directory that should be backed up\n        backup_name: Optional custom name for the backup. If not provided,\n                    a name will be generated using the source directory name\n                    and current timestamp (e.g., \"mydir_20250325_123045\")\n\n    Returns:\n        str: The name of the created backup (either the provided backup_name\n            or the auto-generated name)\n\n    !!! note\n        - If a backup with the specified name already exists in S3, it will be overwritten.\n        - The manifest includes SHA-256 hashes and modification times for all files, which can be used for verification or restoration purposes.\n        - The archive is created in a temporary file before being uploaded to S3.\n\n    Example:\n        ```python\n        backup_manager = S3BackupManager(bucket_name=\"my-bucket\")\n        backup_name = backup_manager.full_backup(\"/path/to/source\")\n        print(f\"Full backup created: {backup_name}\")\n        ```\n    \"\"\"\n    params = FullBackupRequest(source_path=source_path, backup_name=backup_name)\n\n    source_path = os.path.abspath(params.source_path)\n    source_dir = os.path.basename(source_path)\n\n    if not os.path.exists(source_path):\n        raise ValueError(f\"Source path does not exist: {source_path}\")\n\n    if not params.backup_name:\n        date_suffix = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        backup_name = f\"{source_dir}_{date_suffix}\"\n    else:\n        backup_name = params.backup_name\n\n    with tempfile.NamedTemporaryFile(suffix=\".tar.gz\", delete=False) as tmp_file:\n        archive_path = tmp_file.name\n\n    try:\n        with tarfile.open(archive_path, \"w:gz\") as tar:\n            tar.add(source_path, arcname=os.path.basename(source_path))\n\n        backup_key = f\"{self.full_prefix}{backup_name}/full_backup.tar.gz\"\n        self.s3_client.upload_file(\n            archive_path,\n            self.bucket,\n            backup_key,\n            ExtraArgs={\"StorageClass\": \"STANDARD\"},\n        )\n\n        created_at = datetime.now().isoformat()\n\n        manifest = {\n            \"files\": {},\n            \"created_at\": created_at,\n            \"type\": \"full\",\n        }\n\n        file_count = 0\n        for root, _, files in os.walk(source_path):\n            for file in files:\n                file_path = os.path.join(root, file)\n                rel_path = os.path.relpath(file_path, source_path)\n                manifest[\"files\"][rel_path] = {\n                    \"hash\": self._get_file_hash(file_path),\n                    \"mtime\": os.path.getmtime(file_path),\n                }\n                file_count += 1\n\n        self._save_manifest(manifest, backup_name)\n\n        return backup_name\n\n    finally:\n        if os.path.exists(archive_path):\n            os.unlink(archive_path)\n</code></pre>"},{"location":"api/api-reference/#pdldb.S3BackupManager.list_backups","title":"<code>list_backups()</code>","text":"<p>Lists all available backups in the S3 bucket under the configured prefix.</p> <p>This method scans both full backups and mirror backups:</p> <ul> <li>Full backups: S3 prefixes under full_backups/, each with a manifest.json</li> <li>Mirror backup: A single backup in the mirror_backup/ prefix with its own manifest.json</li> </ul> RETURNS DESCRIPTION <code>List[Dict[str, Any]]</code> <p>List[Dict[str, Any]]: A list of dictionaries containing details about each backup: - name: The name of the backup (prefix name for full backups, \"mirror_backup\" for mirror) - type: Either \"full\" or \"mirror\" - created_at: ISO timestamp when the backup was created - source_directory: Original source directory name (only for mirror backups)</p> <p>Note</p> <p>If errors occur while fetching objects or parsing manifests, they are logged to stdout but don't interrupt the listing process. The S3 list operation uses pagination to handle potentially large numbers of backups.</p> Example <pre><code>backup_manager = S3BackupManager(bucket_name=\"my-bucket\")\nbackups = backup_manager.list_backups()\nfor backup in backups:\n    print(f\"{backup['name']} ({backup['type']}): {backup['created_at']}\")\n</code></pre> Source code in <code>src/pdldb/s3_backup_manager.py</code> <pre><code>def list_backups(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Lists all available backups in the S3 bucket under the configured prefix.\n\n    This method scans both full backups and mirror backups:\n\n    - Full backups: S3 prefixes under full_backups/, each with a manifest.json\n    - Mirror backup: A single backup in the mirror_backup/ prefix with its own manifest.json\n\n    Returns:\n        List[Dict[str, Any]]: A list of dictionaries containing details about each backup:\n            - name: The name of the backup (prefix name for full backups, \"mirror_backup\" for mirror)\n            - type: Either \"full\" or \"mirror\"\n            - created_at: ISO timestamp when the backup was created\n            - source_directory: Original source directory name (only for mirror backups)\n\n    !!! note\n        If errors occur while fetching objects or parsing manifests, they are logged to stdout\n        but don't interrupt the listing process. The S3 list operation uses pagination to handle\n        potentially large numbers of backups.\n\n    Example:\n        ```python\n        backup_manager = S3BackupManager(bucket_name=\"my-bucket\")\n        backups = backup_manager.list_backups()\n        for backup in backups:\n            print(f\"{backup['name']} ({backup['type']}): {backup['created_at']}\")\n        ```\n    \"\"\"\n    backups = []\n\n    try:\n        response = self.s3_client.list_objects_v2(\n            Bucket=self.bucket, Prefix=self.full_prefix, Delimiter=\"/\"\n        )\n\n        for prefix in response.get(\"CommonPrefixes\", []):\n            backup_name = prefix[\"Prefix\"].split(\"/\")[-2]\n            try:\n                manifest = self._load_manifest(backup_name)\n                backups.append(\n                    {\n                        \"name\": backup_name,\n                        \"type\": \"full\",\n                        \"created_at\": manifest.get(\"created_at\", \"unknown\"),\n                    }\n                )\n            except ClientError as e:\n                print(f\"Error loading manifest for {backup_name}: {e}\")\n            except Exception as e:\n                print(f\"Unexpected error processing backup {backup_name}: {e}\")\n\n    except ClientError as e:\n        print(f\"Error listing full backups: {e}\")\n\n    try:\n        manifest = self._load_manifest()\n        if manifest.get(\"files\"):\n            backups.append(\n                {\n                    \"name\": \"mirror_backup\",\n                    \"type\": \"mirror\",\n                    \"created_at\": manifest.get(\"created_at\", \"unknown\"),\n                    \"source_directory\": manifest.get(\"source_directory\", \"unknown\"),\n                }\n            )\n    except ClientError as e:\n        print(f\"Error loading mirror backup manifest: {e}\")\n    except Exception as e:\n        print(f\"Unexpected error processing mirror backup: {e}\")\n\n    return backups\n</code></pre>"},{"location":"api/api-reference/#pdldb.S3BackupManager.mirror_backup","title":"<code>mirror_backup(source_path)</code>","text":"<p>Creates or updates a mirror backup from the source directory to S3.</p> <p>A mirror backup differs from a full backup in several ways:</p> <ul> <li>Only one mirror backup can exist at a time (in the mirror_backup prefix)</li> <li>Files are stored individually rather than in a tar archive</li> <li>Only files that have changed (based on hash comparison) are uploaded</li> <li>Files in S3 that no longer exist in the source are removed</li> </ul> <p>The method performs an incremental sync by:</p> <ol> <li>Scanning the source directory and calculating file hashes</li> <li>Comparing with the current S3 mirror backup manifest</li> <li>Determining which files need to be added, updated, or removed</li> <li>Uploading only the changed files and updating the manifest</li> </ol> PARAMETER DESCRIPTION <code>source_path</code> <p>Path to the directory that should be backed up</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Always returns \"mirror_backup\" as the backup identifier</p> <p> TYPE: <code>str</code> </p> <p>Note</p> <ul> <li>The backup's source directory name is stored in the manifest to help with restoration</li> <li>S3 objects are deleted in batches of 1000 (S3 API limitation)</li> <li>Errors during upload or deletion are logged but don't interrupt the overall process</li> </ul> Example <pre><code>backup_manager = S3BackupManager(bucket_name=\"my-bucket\")\nsuccess = backup_manager.mirror_backup(\"/path/to/source\")\nif success:\n    print(\"Mirror backup completed!\")\n</code></pre> Source code in <code>src/pdldb/s3_backup_manager.py</code> <pre><code>def mirror_backup(self, source_path: str) -&gt; str:\n    \"\"\"\n    Creates or updates a mirror backup from the source directory to S3.\n\n    A mirror backup differs from a full backup in several ways:\n\n    - Only one mirror backup can exist at a time (in the mirror_backup prefix)\n    - Files are stored individually rather than in a tar archive\n    - Only files that have changed (based on hash comparison) are uploaded\n    - Files in S3 that no longer exist in the source are removed\n\n    The method performs an incremental sync by:\n\n    1. Scanning the source directory and calculating file hashes\n    2. Comparing with the current S3 mirror backup manifest\n    3. Determining which files need to be added, updated, or removed\n    4. Uploading only the changed files and updating the manifest\n\n    Args:\n        source_path: Path to the directory that should be backed up\n\n    Returns:\n        str: Always returns \"mirror_backup\" as the backup identifier\n\n    !!! note\n        - The backup's source directory name is stored in the manifest to help with restoration\n        - S3 objects are deleted in batches of 1000 (S3 API limitation)\n        - Errors during upload or deletion are logged but don't interrupt the overall process\n\n    Example:\n        ```python\n        backup_manager = S3BackupManager(bucket_name=\"my-bucket\")\n        success = backup_manager.mirror_backup(\"/path/to/source\")\n        if success:\n            print(\"Mirror backup completed!\")\n        ```\n    \"\"\"\n    params = MirrorBackupRequest(source_path=source_path)\n    source_path = os.path.abspath(params.source_path)\n\n    if not os.path.exists(source_path):\n        raise ValueError(f\"Source path does not exist: {source_path}\")\n\n    source_dir = os.path.basename(source_path)\n\n    try:\n        current_mir_manifest = self._load_manifest()\n    except ClientError:\n        current_mir_manifest = {\n            \"files\": {},\n            \"created_at\": datetime.now().isoformat(),\n        }\n\n    s3_files = {}\n    try:\n        paginator = self.s3_client.get_paginator(\"list_objects_v2\")\n        pages = paginator.paginate(Bucket=self.bucket, Prefix=self.mir_prefix)\n\n        for page in pages:\n            for obj in page.get(\"Contents\", []):\n                key = obj[\"Key\"]\n                if not key.endswith(\"manifest.json\"):\n                    rel_path = key[len(self.mir_prefix) :]\n                    s3_files[rel_path] = key\n    except ClientError:\n        pass\n\n    local_files = {}\n    for root, _, files in os.walk(source_path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            rel_path = os.path.relpath(file_path, source_path)\n            local_files[rel_path] = {\n                \"path\": file_path,\n                \"hash\": self._get_file_hash(file_path),\n                \"mtime\": os.path.getmtime(file_path),\n            }\n\n    to_upload = []\n    to_delete = []\n\n    for rel_path, file_info in local_files.items():\n        if rel_path not in current_mir_manifest.get(\"files\", {}) or file_info[\n            \"hash\"\n        ] != current_mir_manifest[\"files\"].get(rel_path, {}).get(\"hash\"):\n            to_upload.append((file_info[\"path\"], rel_path))\n\n    for rel_path in s3_files:\n        if rel_path not in local_files:\n            to_delete.append({\"Key\": s3_files[rel_path]})\n\n    new_manifest = {\n        \"files\": {},\n        \"created_at\": datetime.now().isoformat(),\n        \"type\": \"mirror\",\n        \"source_directory\": source_dir,\n    }\n\n    for rel_path, file_info in local_files.items():\n        new_manifest[\"files\"][rel_path] = {\n            \"hash\": file_info[\"hash\"],\n            \"mtime\": file_info[\"mtime\"],\n        }\n\n    for file_path, rel_path in to_upload:\n        target_key = f\"{self.mir_prefix}{rel_path}\"\n        try:\n            self.s3_client.upload_file(\n                file_path,\n                self.bucket,\n                target_key,\n                ExtraArgs={\"StorageClass\": \"STANDARD\"},\n            )\n        except Exception as e:\n            print(f\"Error uploading {file_path}: {e}\")\n\n    if to_delete:\n        try:\n            for i in range(0, len(to_delete), 1000):\n                batch = to_delete[i : i + 1000]\n                self.s3_client.delete_objects(\n                    Bucket=self.bucket, Delete={\"Objects\": batch, \"Quiet\": True}\n                )\n        except Exception as e:\n            print(f\"Error deleting objects: {e}\")\n\n    self._save_manifest(new_manifest)\n\n    return \"mirror_backup\"\n</code></pre>"},{"location":"api/api-reference/#pdldb.S3BackupManager.restore","title":"<code>restore(backup_name, destination_path, specific_files=None)</code>","text":"<p>Restores files from an S3 backup to a specified local destination path.</p> <p>This method supports restoring from both full and mirror backups:</p> <ul> <li>For mirror backups: Files are downloaded individually while preserving the directory structure</li> <li>For full backups: The tar.gz archive is downloaded and extracted to the destination</li> </ul> PARAMETER DESCRIPTION <code>backup_name</code> <p>The name of the backup to restore from. Use \"mirror_backup\" for mirror backups         or the directory name for full backups.</p> <p> TYPE: <code>str</code> </p> <code>destination_path</code> <p>The target directory where files will be restored to.</p> <p> TYPE: <code>str</code> </p> <code>specific_files</code> <p>Optional list of specific file paths to restore. If None, all files              will be restored. Paths should be relative to the original backup source.</p> <p> TYPE: <code>Optional[List[str]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>bool</code> <p>True if restoration succeeded, False if it failed.</p> <p> TYPE: <code>bool</code> </p> <p>Note</p> <ul> <li>For mirror backups, if the original source directory is stored in the manifest, a subdirectory with that name will be created at the destination.</li> <li>For full backups, the entire archive is downloaded to a temporary directory before extracting the specified files or the entire contents.</li> <li>Any errors during restoration are logged to stdout and will cause the method to return False.</li> </ul> Example <pre><code>backup_manager = S3BackupManager(bucket_name=\"my-bucket\")\nsuccess = backup_manager.restore(\"my_backup\", \"/restore/path\")\nif success:\n    print(\"Restoration successful!\")\n</code></pre> Source code in <code>src/pdldb/s3_backup_manager.py</code> <pre><code>def restore(\n    self,\n    backup_name: str,\n    destination_path: str,\n    specific_files: Optional[List[str]] = None,\n) -&gt; bool:\n    \"\"\"\n    Restores files from an S3 backup to a specified local destination path.\n\n    This method supports restoring from both full and mirror backups:\n\n    - For mirror backups: Files are downloaded individually while preserving the directory structure\n    - For full backups: The tar.gz archive is downloaded and extracted to the destination\n\n    Args:\n        backup_name: The name of the backup to restore from. Use \"mirror_backup\" for mirror backups\n                    or the directory name for full backups.\n        destination_path: The target directory where files will be restored to.\n        specific_files: Optional list of specific file paths to restore. If None, all files \n                        will be restored. Paths should be relative to the original backup source.\n\n    Returns:\n        bool: True if restoration succeeded, False if it failed.\n\n    !!! note\n        - For mirror backups, if the original source directory is stored in the manifest, a subdirectory with that name will be created at the destination.\n        - For full backups, the entire archive is downloaded to a temporary directory before extracting the specified files or the entire contents.\n        - Any errors during restoration are logged to stdout and will cause the method to return False.\n\n    Example:\n        ```python\n        backup_manager = S3BackupManager(bucket_name=\"my-bucket\")\n        success = backup_manager.restore(\"my_backup\", \"/restore/path\")\n        if success:\n            print(\"Restoration successful!\")\n        ```\n    \"\"\"\n    params = RestoreRequest(\n        backup_name=backup_name,\n        destination_path=destination_path,\n        specific_files=specific_files,\n    )\n\n    destination_path = params.destination_path\n    backup_name = params.backup_name\n    specific_files = params.specific_files\n\n    os.makedirs(destination_path, exist_ok=True)\n\n    if backup_name == \"mirror_backup\":\n        manifest = self._load_manifest()\n        files_prefix = self.mir_prefix\n\n        source_dir = manifest.get(\"source_directory\", \"\")\n\n        if source_dir:\n            target_path = os.path.join(destination_path, source_dir)\n            os.makedirs(target_path, exist_ok=True)\n        else:\n            target_path = destination_path\n\n        try:\n            paginator = self.s3_client.get_paginator(\"list_objects_v2\")\n            pages = paginator.paginate(Bucket=self.bucket, Prefix=files_prefix)\n\n            for page in pages:\n                for obj in page.get(\"Contents\", []):\n                    file_key = obj[\"Key\"]\n                    if file_key.endswith(\"manifest.json\"):\n                        continue\n\n                    rel_path = file_key[len(files_prefix) :]\n\n                    if specific_files and rel_path not in specific_files:\n                        continue\n\n                    if source_dir:\n                        local_path = os.path.join(target_path, rel_path)\n                    else:\n                        local_path = os.path.join(destination_path, rel_path)\n\n                    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n\n                    self.s3_client.download_file(self.bucket, file_key, local_path)\n\n            return True\n        except ClientError as e:\n            print(f\"Error during mirror restore: {e}\")\n            return False\n\n    manifest = self._load_manifest(backup_name)\n\n    with tempfile.TemporaryDirectory() as temp_dir:\n        archive_key = f\"{self.full_prefix}{backup_name}/full_backup.tar.gz\"\n        archive_path = os.path.join(temp_dir, f\"{backup_name}.tar.gz\")\n\n        try:\n            self.s3_client.download_file(self.bucket, archive_key, archive_path)\n\n            with tarfile.open(archive_path, \"r:gz\") as tar:\n                if specific_files:\n                    for member in tar.getmembers():\n                        if member.name in specific_files:\n                            tar.extract(member, path=destination_path)\n                else:\n                    tar.extractall(path=destination_path)\n\n            return True\n\n        except ClientError:\n            return False\n\n    return False\n</code></pre>"}]}